{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNetV2 for CIFAR for PGD",
      "provenance": [],
      "collapsed_sections": [
        "YX3ojIoKCQa1",
        "K4mwCDxR3otR",
        "O0zBGmj4BIE4"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaoshiang/MobileNetV2-CIFAR-Cleverhans/blob/master/MobileNetV2_for_CIFAR_for_PGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fus01p5tCEmG",
        "colab_type": "text"
      },
      "source": [
        "# Applying Cleverhans to a version of MobileNetV2.\n",
        "\n",
        "We specifically choose a modern architecture to analyze. A CNN built solely on convolutions and pooling are by now far out of date. We specifically wanted to analyze a CNN with batchnorm, bottlenecks, and residual blocks. \n",
        "\n",
        "But we can't afford to train on ImageNet so we decided build for CIFAR10. This required us to shorten MobileNetV2 to prevent overstriding down of the feature maps down to 1x1 degenerate maps. \n",
        "\n",
        "https://arxiv.org/abs/1911.09665\n",
        "\n",
        "How Does Batch Normalization Help Optimization?\n",
        "https://arxiv.org/pdf/1805.11604.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX3ojIoKCQa1",
        "colab_type": "text"
      },
      "source": [
        "### Setup libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ThssnZ0KLV-",
        "colab_type": "code",
        "outputId": "50941a0d-f7fd-49f6-d465-23b37b50898f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "%tensorflow_version 1.14\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `1.14`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RTVj-bR6Udn",
        "colab_type": "code",
        "outputId": "19b0b6e1-6825-45da-8bbd-b0b9d4eb4d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import keras\n",
        "# print(keras.__version__)\n",
        "print(tensorflow.keras.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySXDM4FB_Wy8",
        "colab_type": "code",
        "outputId": "8388d2b7-7d61-4079-d587-526f80d40cb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 16914428360906805188\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 13257041665067616516\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 8111800764023474361\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14912199066\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 17830433992289389595\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIoZLWj6JQPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -qq -e git+http://github.com/yaoshiang/cleverhans.git#egg=cleverhans\n",
        "import sys\n",
        "sys.path.append('/content/src/cleverhans')\n",
        "import cleverhans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGtv7kFh_dRJ",
        "colab_type": "code",
        "outputId": "9407354e-5bb4-4e2b-fc99-a93bf807a13b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jan 21 21:36:58 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0    30W /  70W |    111MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgT1GvjkN6_z",
        "colab_type": "code",
        "outputId": "702842e2-199c-4ea6-9d25-ded20ee7773f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import cleverhans_tutorials as ct\n",
        "import cleverhans_tutorials.cifar10_tutorial_tf as ctc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/src/cleverhans/cleverhans/utils_tf.py:345: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zfn0DEyyrxN",
        "colab_type": "text"
      },
      "source": [
        "### Create a cleverhans wrapper for the Keras version of MobileNet v2.\n",
        "\n",
        "We use MobileNet V2 as an example of \"modern\" image classifier architecture, which includes Residual Blocks and batchnorms. The next generation after that such as MobileNetV3 are generally discovered via AutoML searches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwAgrBlXHWuh",
        "colab_type": "text"
      },
      "source": [
        "### First lift out of a lot of Keras source code and modify the Keras implementation of MobileNetV2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGFZ2MxT2tsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correct_pad(backend, inputs, kernel_size):\n",
        "    \"\"\"Returns a tuple for zero-padding for 2D convolution with downsampling.\n",
        "    # Arguments\n",
        "        input_size: An integer or tuple/list of 2 integers.\n",
        "        kernel_size: An integer or tuple/list of 2 integers.\n",
        "    # Returns\n",
        "        A tuple.\n",
        "    \"\"\"\n",
        "    img_dim = 2 if backend.image_data_format() == 'channels_first' else 1\n",
        "    input_size = backend.int_shape(inputs)[img_dim:(img_dim + 2)]\n",
        "\n",
        "    if isinstance(kernel_size, int):\n",
        "        kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "    if input_size[0] is None:\n",
        "        adjust = (1, 1)\n",
        "    else:\n",
        "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
        "\n",
        "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
        "\n",
        "    return ((correct[0] - adjust[0], correct[0]),\n",
        "            (correct[1] - adjust[1], correct[1]))\n",
        "    \n",
        "\n",
        "    \"\"\"MobileNet v2 models for Keras.\n",
        "MobileNetV2 is a general architecture and can be used for multiple use cases.\n",
        "Depending on the use case, it can use different input layer size and\n",
        "different width factors. This allows different width models to reduce\n",
        "the number of multiply-adds and thereby\n",
        "reduce inference cost on mobile devices.\n",
        "MobileNetV2 is very similar to the original MobileNet,\n",
        "except that it uses inverted residual blocks with\n",
        "bottlenecking features. It has a drastically lower\n",
        "parameter count than the original MobileNet.\n",
        "MobileNets support any input size greater\n",
        "than 32 x 32, with larger image sizes\n",
        "offering better performance.\n",
        "The number of parameters and number of multiply-adds\n",
        "can be modified by using the `alpha` parameter,\n",
        "which increases/decreases the number of filters in each layer.\n",
        "By altering the image size and `alpha` parameter,\n",
        "all 22 models from the paper can be built, with ImageNet weights provided.\n",
        "The paper demonstrates the performance of MobileNets using `alpha` values of\n",
        "1.0 (also called 100 % MobileNet), 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4\n",
        "For each of these `alpha` values, weights for 5 different input image sizes\n",
        "are provided (224, 192, 160, 128, and 96).\n",
        "The following table describes the performance of\n",
        "MobileNet on various input sizes:\n",
        "------------------------------------------------------------------------\n",
        "MACs stands for Multiply Adds\n",
        " Classification Checkpoint| MACs (M) | Parameters (M)| Top 1 Accuracy| Top 5 Accuracy\n",
        "--------------------------|------------|---------------|---------|----|-------------\n",
        "| [mobilenet_v2_1.4_224]  | 582 | 6.06 |          75.0 | 92.5 |\n",
        "| [mobilenet_v2_1.3_224]  | 509 | 5.34 |          74.4 | 92.1 |\n",
        "| [mobilenet_v2_1.0_224]  | 300 | 3.47 |          71.8 | 91.0 |\n",
        "| [mobilenet_v2_1.0_192]  | 221 | 3.47 |          70.7 | 90.1 |\n",
        "| [mobilenet_v2_1.0_160]  | 154 | 3.47 |          68.8 | 89.0 |\n",
        "| [mobilenet_v2_1.0_128]  | 99  | 3.47 |          65.3 | 86.9 |\n",
        "| [mobilenet_v2_1.0_96]   | 56  | 3.47 |          60.3 | 83.2 |\n",
        "| [mobilenet_v2_0.75_224] | 209 | 2.61 |          69.8 | 89.6 |\n",
        "| [mobilenet_v2_0.75_192] | 153 | 2.61 |          68.7 | 88.9 |\n",
        "| [mobilenet_v2_0.75_160] | 107 | 2.61 |          66.4 | 87.3 |\n",
        "| [mobilenet_v2_0.75_128] | 69  | 2.61 |          63.2 | 85.3 |\n",
        "| [mobilenet_v2_0.75_96]  | 39  | 2.61 |          58.8 | 81.6 |\n",
        "| [mobilenet_v2_0.5_224]  | 97  | 1.95 |          65.4 | 86.4 |\n",
        "| [mobilenet_v2_0.5_192]  | 71  | 1.95 |          63.9 | 85.4 |\n",
        "| [mobilenet_v2_0.5_160]  | 50  | 1.95 |          61.0 | 83.2 |\n",
        "| [mobilenet_v2_0.5_128]  | 32  | 1.95 |          57.7 | 80.8 |\n",
        "| [mobilenet_v2_0.5_96]   | 18  | 1.95 |          51.2 | 75.8 |\n",
        "| [mobilenet_v2_0.35_224] | 59  | 1.66 |          60.3 | 82.9 |\n",
        "| [mobilenet_v2_0.35_192] | 43  | 1.66 |          58.2 | 81.2 |\n",
        "| [mobilenet_v2_0.35_160] | 30  | 1.66 |          55.7 | 79.1 |\n",
        "| [mobilenet_v2_0.35_128] | 20  | 1.66 |          50.8 | 75.0 |\n",
        "| [mobilenet_v2_0.35_96]  | 11  | 1.66 |          45.5 | 70.4 |\n",
        "The weights for all 16 models are obtained and\n",
        "translated from the Tensorflow checkpoints\n",
        "from TensorFlow checkpoints found [here]\n",
        "(https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md).\n",
        "# Reference\n",
        "This file contains building code for MobileNetV2, based on\n",
        "[MobileNetV2: Inverted Residuals and Linear Bottlenecks]\n",
        "(https://arxiv.org/abs/1801.04381) (CVPR 2018)\n",
        "Tests comparing this model to the existing Tensorflow model can be\n",
        "found at [mobilenet_v2_keras]\n",
        "(https://github.com/JonathanCMitchell/mobilenet_v2_keras)\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "# TODO Change path to v1.1\n",
        "BASE_WEIGHT_PATH = ('https://github.com/JonathanCMitchell/mobilenet_v2_keras/'\n",
        "                    'releases/download/v1.1/')\n",
        "\n",
        "backend = None\n",
        "layers = None\n",
        "models = None\n",
        "keras_utils = None\n",
        "\n",
        "\n",
        "_KERAS_BACKEND = None\n",
        "_KERAS_LAYERS = None\n",
        "_KERAS_MODELS = None\n",
        "_KERAS_UTILS = None\n",
        "\n",
        "\n",
        "def get_submodules_from_kwargs(kwargs):\n",
        "    backend = kwargs.get('backend', _KERAS_BACKEND)\n",
        "    layers = kwargs.get('layers', _KERAS_LAYERS)\n",
        "    models = kwargs.get('models', _KERAS_MODELS)\n",
        "    utils = kwargs.get('utils', _KERAS_UTILS)\n",
        "    for key in kwargs.keys():\n",
        "        if key not in ['backend', 'layers', 'models', 'utils']:\n",
        "            raise TypeError('Invalid keyword argument: %s', key)\n",
        "    return backend, layers, models, utils\n",
        "\n",
        "\n",
        "\n",
        "# This function is taken from the original tf repo.\n",
        "# It ensures that all layers have a channel number that is divisible by 8\n",
        "# It can be seen here:\n",
        "# https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "def keras_modules_injection(base_fun):\n",
        "\n",
        "    def wrapper(*args, **kwargs):\n",
        "        kwargs['backend'] = backend\n",
        "        kwargs['layers'] = layers\n",
        "        kwargs['models'] = models\n",
        "        kwargs['utils'] = utils\n",
        "        return base_fun(*args, **kwargs)\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "@keras_modules_injection\n",
        "def MobileNetV2(input_shape=None,\n",
        "                alpha=1.0,\n",
        "                include_top=True,\n",
        "                weights='imagenet',\n",
        "                input_tensor=None,\n",
        "                pooling=None,\n",
        "                classes=1000,\n",
        "                **kwargs):\n",
        "    \"\"\"Instantiates the MobileNetV2 architecture.\n",
        "    # Arguments\n",
        "        input_shape: optional shape tuple, to be specified if you would\n",
        "            like to use a model with an input img resolution that is not\n",
        "            (224, 224, 3).\n",
        "            It should have exactly 3 inputs channels (224, 224, 3).\n",
        "            You can also omit this option if you would like\n",
        "            to infer input_shape from an input_tensor.\n",
        "            If you choose to include both input_tensor and input_shape then\n",
        "            input_shape will be used if they match, if the shapes\n",
        "            do not match then we will throw an error.\n",
        "            E.g. `(160, 160, 3)` would be one valid value.\n",
        "        alpha: controls the width of the network. This is known as the\n",
        "        width multiplier in the MobileNetV2 paper, but the name is kept for\n",
        "        consistency with MobileNetV1 in Keras.\n",
        "            - If `alpha` < 1.0, proportionally decreases the number\n",
        "                of filters in each layer.\n",
        "            - If `alpha` > 1.0, proportionally increases the number\n",
        "                of filters in each layer.\n",
        "            - If `alpha` = 1, default number of filters from the paper\n",
        "                 are used at each layer.\n",
        "        include_top: whether to include the fully-connected\n",
        "            layer at the top of the network.\n",
        "        weights: one of `None` (random initialization),\n",
        "              'imagenet' (pre-training on ImageNet),\n",
        "              or the path to the weights file to be loaded.\n",
        "        input_tensor: optional Keras tensor (i.e. output of\n",
        "            `layers.Input()`)\n",
        "            to use as image input for the model.\n",
        "        pooling: Optional pooling mode for feature extraction\n",
        "            when `include_top` is `False`.\n",
        "            - `None` means that the output of the model\n",
        "                will be the 4D tensor output of the\n",
        "                last convolutional block.\n",
        "            - `avg` means that global average pooling\n",
        "                will be applied to the output of the\n",
        "                last convolutional block, and thus\n",
        "                the output of the model will be a\n",
        "                2D tensor.\n",
        "            - `max` means that global max pooling will\n",
        "                be applied.\n",
        "        classes: optional number of classes to classify images\n",
        "            into, only to be specified if `include_top` is True, and\n",
        "            if no `weights` argument is specified.\n",
        "    # Returns\n",
        "        A Keras model instance.\n",
        "    # Raises\n",
        "        ValueError: in case of invalid argument for `weights`,\n",
        "            or invalid input shape or invalid alpha, rows when\n",
        "            weights='imagenet'\n",
        "    \"\"\"\n",
        "    global backend, layers, models, keras_utils\n",
        "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
        "\n",
        "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization), `imagenet` '\n",
        "                         '(pre-training on ImageNet), '\n",
        "                         'or the path to the weights file to be loaded.')\n",
        "\n",
        "    if weights == 'imagenet' and include_top and classes != 1000:\n",
        "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top` '\n",
        "                         'as true, `classes` should be 1000')\n",
        "\n",
        "    # Determine proper input shape and default size.\n",
        "    # If both input_shape and input_tensor are used, they should match\n",
        "    if input_shape is not None and input_tensor is not None:\n",
        "        try:\n",
        "            is_input_t_tensor = backend.is_keras_tensor(input_tensor)\n",
        "        except ValueError:\n",
        "            try:\n",
        "                is_input_t_tensor = backend.is_keras_tensor(\n",
        "                    keras_utils.get_source_inputs(input_tensor))\n",
        "            except ValueError:\n",
        "                raise ValueError('input_tensor: ', input_tensor,\n",
        "                                 'is not type input_tensor')\n",
        "        if is_input_t_tensor:\n",
        "            if backend.image_data_format == 'channels_first':\n",
        "                if backend.int_shape(input_tensor)[1] != input_shape[1]:\n",
        "                    raise ValueError('input_shape: ', input_shape,\n",
        "                                     'and input_tensor: ', input_tensor,\n",
        "                                     'do not meet the same shape requirements')\n",
        "            else:\n",
        "                if backend.int_shape(input_tensor)[2] != input_shape[1]:\n",
        "                    raise ValueError('input_shape: ', input_shape,\n",
        "                                     'and input_tensor: ', input_tensor,\n",
        "                                     'do not meet the same shape requirements')\n",
        "        else:\n",
        "            raise ValueError('input_tensor specified: ', input_tensor,\n",
        "                             'is not a keras tensor')\n",
        "\n",
        "    # If input_shape is None, infer shape from input_tensor\n",
        "    if input_shape is None and input_tensor is not None:\n",
        "\n",
        "        try:\n",
        "            backend.is_keras_tensor(input_tensor)\n",
        "        except ValueError:\n",
        "            raise ValueError('input_tensor: ', input_tensor,\n",
        "                             'is type: ', type(input_tensor),\n",
        "                             'which is not a valid type')\n",
        "\n",
        "        if input_shape is None and not backend.is_keras_tensor(input_tensor):\n",
        "            default_size = 224\n",
        "        elif input_shape is None and backend.is_keras_tensor(input_tensor):\n",
        "            if backend.image_data_format() == 'channels_first':\n",
        "                rows = backend.int_shape(input_tensor)[2]\n",
        "                cols = backend.int_shape(input_tensor)[3]\n",
        "            else:\n",
        "                rows = backend.int_shape(input_tensor)[1]\n",
        "                cols = backend.int_shape(input_tensor)[2]\n",
        "\n",
        "            if rows == cols and rows in [96, 128, 160, 192, 224]:\n",
        "                default_size = rows\n",
        "            else:\n",
        "                default_size = 224\n",
        "\n",
        "    # If input_shape is None and no input_tensor\n",
        "    elif input_shape is None:\n",
        "        default_size = 224\n",
        "\n",
        "    # If input_shape is not None, assume default size\n",
        "    else:\n",
        "        if backend.image_data_format() == 'channels_first':\n",
        "            rows = input_shape[1]\n",
        "            cols = input_shape[2]\n",
        "        else:\n",
        "            rows = input_shape[0]\n",
        "            cols = input_shape[1]\n",
        "\n",
        "        if rows == cols and rows in [96, 128, 160, 192, 224]:\n",
        "            default_size = rows\n",
        "        else:\n",
        "            default_size = 224\n",
        "\n",
        "    input_shape = _obtain_input_shape(input_shape,\n",
        "                                      default_size=default_size,\n",
        "                                      min_size=32,\n",
        "                                      data_format=backend.image_data_format(),\n",
        "                                      require_flatten=include_top,\n",
        "                                      weights=weights)\n",
        "\n",
        "    if backend.image_data_format() == 'channels_last':\n",
        "        row_axis, col_axis = (0, 1)\n",
        "    else:\n",
        "        row_axis, col_axis = (1, 2)\n",
        "    rows = input_shape[row_axis]\n",
        "    cols = input_shape[col_axis]\n",
        "\n",
        "    if weights == 'imagenet':\n",
        "        if alpha not in [0.35, 0.50, 0.75, 1.0, 1.3, 1.4]:\n",
        "            raise ValueError('If imagenet weights are being loaded, '\n",
        "                             'alpha can be one of `0.35`, `0.50`, `0.75`, '\n",
        "                             '`1.0`, `1.3` or `1.4` only.')\n",
        "\n",
        "        if rows != cols or rows not in [96, 128, 160, 192, 224]:\n",
        "            rows = 224\n",
        "            warnings.warn('`input_shape` is undefined or non-square, '\n",
        "                          'or `rows` is not in [96, 128, 160, 192, 224].'\n",
        "                          ' Weights for input shape (224, 224) will be'\n",
        "                          ' loaded as the default.')\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = layers.Input(shape=input_shape)\n",
        "    else:\n",
        "        if not backend.is_keras_tensor(input_tensor):\n",
        "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    channel_axis = 1 if backend.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    first_block_filters = _make_divisible(32 * alpha, 8)\n",
        "    x = layers.ZeroPadding2D(padding=correct_pad(backend, img_input, 3),\n",
        "                             name='Conv1_pad')(img_input)\n",
        "    x = layers.Conv2D(first_block_filters,\n",
        "                      kernel_size=3,\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      use_bias=False,\n",
        "                      name='Conv1')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                  epsilon=1e-3,\n",
        "                                  momentum=0.999,\n",
        "                                  name='bn_Conv1')(x)\n",
        "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n",
        "                            expansion=1, block_id=0)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=1)\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=2)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=3)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=4)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=5)\n",
        "\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,\n",
        "    #                         expansion=6, block_id=6)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=7)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=8)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=9)\n",
        "\n",
        "    # x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=10)\n",
        "    # x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=11)\n",
        "    # x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=12)\n",
        "\n",
        "    # x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2,\n",
        "    #                         expansion=6, block_id=13)\n",
        "    # x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=14)\n",
        "    # x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=15)\n",
        "\n",
        "    # x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=16)\n",
        "\n",
        "    # no alpha applied to last conv as stated in the paper:\n",
        "    # if the width multiplier is greater than 1 we\n",
        "    # increase the number of output channels\n",
        "    if alpha > 1.0:\n",
        "        last_block_filters = _make_divisible(1280 * alpha, 8)\n",
        "    else:\n",
        "        last_block_filters = 1280\n",
        "\n",
        "    x = layers.Conv2D(last_block_filters,\n",
        "                      kernel_size=1,\n",
        "                      use_bias=False,\n",
        "                      name='Conv_1')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                  epsilon=1e-3,\n",
        "                                  momentum=0.999,\n",
        "                                  name='Conv_1_bn')(x)\n",
        "    x = layers.ReLU(6., name='out_relu')(x)\n",
        "\n",
        "    if include_top:\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "        x = layers.Dense(classes, activation='softmax',\n",
        "                         use_bias=True, name='Logits')(x)\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = layers.GlobalAveragePooling2D()(x)\n",
        "        elif pooling == 'max':\n",
        "            x = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = models.Model(inputs, x,\n",
        "                         name='mobilenetv2_%0.2f_%s' % (alpha, rows))\n",
        "\n",
        "    # Load weights.\n",
        "    if weights == 'imagenet':\n",
        "        if include_top:\n",
        "            model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' +\n",
        "                          str(alpha) + '_' + str(rows) + '.h5')\n",
        "            weight_path = BASE_WEIGHT_PATH + model_name\n",
        "            weights_path = keras_utils.get_file(\n",
        "                model_name, weight_path, cache_subdir='models')\n",
        "        else:\n",
        "            model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' +\n",
        "                          str(alpha) + '_' + str(rows) + '_no_top' + '.h5')\n",
        "            weight_path = BASE_WEIGHT_PATH + model_name\n",
        "            weights_path = keras_utils.get_file(\n",
        "                model_name, weight_path, cache_subdir='models')\n",
        "        model.load_weights(weights_path)\n",
        "    elif weights is not None:\n",
        "        model.load_weights(weights)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
        "    channel_axis = 1 if backend.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    in_channels = backend.int_shape(inputs)[channel_axis]\n",
        "    pointwise_conv_filters = int(filters * alpha)\n",
        "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
        "    x = inputs\n",
        "    prefix = 'block_{}_'.format(block_id)\n",
        "\n",
        "    if block_id:\n",
        "        # Expand\n",
        "        x = layers.Conv2D(expansion * in_channels,\n",
        "                          kernel_size=1,\n",
        "                          padding='same',\n",
        "                          use_bias=False,\n",
        "                          activation=None,\n",
        "                          name=prefix + 'expand')(x)\n",
        "        x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                      epsilon=1e-3,\n",
        "                                      momentum=0.999,\n",
        "                                      name=prefix + 'expand_BN')(x)\n",
        "        x = layers.ReLU(6., name=prefix + 'expand_relu')(x)\n",
        "    else:\n",
        "        prefix = 'expanded_conv_'\n",
        "\n",
        "    # Depthwise\n",
        "    if stride == 2:\n",
        "        x = layers.ZeroPadding2D(padding=correct_pad(backend, x, 3),\n",
        "                                 name=prefix + 'pad')(x)\n",
        "    x = layers.DepthwiseConv2D(kernel_size=3,\n",
        "                               strides=stride,\n",
        "                               activation=None,\n",
        "                               use_bias=False,\n",
        "                               padding='same' if stride == 1 else 'valid',\n",
        "                               name=prefix + 'depthwise')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                  epsilon=1e-3,\n",
        "                                  momentum=0.999,\n",
        "                                  name=prefix + 'depthwise_BN')(x)\n",
        "\n",
        "    x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
        "\n",
        "    # Project\n",
        "    x = layers.Conv2D(pointwise_filters,\n",
        "                      kernel_size=1,\n",
        "                      padding='same',\n",
        "                      use_bias=False,\n",
        "                      activation=None,\n",
        "                      name=prefix + 'project')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                  epsilon=1e-3,\n",
        "                                  momentum=0.999,\n",
        "                                  name=prefix + 'project_BN')(x)\n",
        "\n",
        "    if in_channels == pointwise_filters and stride == 1:\n",
        "        return layers.Add(name=prefix + 'add')([inputs, x])\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def _obtain_input_shape(input_shape,\n",
        "                        default_size,\n",
        "                        min_size,\n",
        "                        data_format,\n",
        "                        require_flatten,\n",
        "                        weights=None):\n",
        "    \"\"\"Internal utility to compute/validate a model's input shape.\n",
        "    # Arguments\n",
        "        input_shape: Either None (will return the default network input shape),\n",
        "            or a user-provided shape to be validated.\n",
        "        default_size: Default input width/height for the model.\n",
        "        min_size: Minimum input width/height accepted by the model.\n",
        "        data_format: Image data format to use.\n",
        "        require_flatten: Whether the model is expected to\n",
        "            be linked to a classifier via a Flatten layer.\n",
        "        weights: One of `None` (random initialization)\n",
        "            or 'imagenet' (pre-training on ImageNet).\n",
        "            If weights='imagenet' input channels must be equal to 3.\n",
        "    # Returns\n",
        "        An integer shape tuple (may include None entries).\n",
        "    # Raises\n",
        "        ValueError: In case of invalid argument values.\n",
        "    \"\"\"\n",
        "    if weights != 'imagenet' and input_shape and len(input_shape) == 3:\n",
        "        if data_format == 'channels_first':\n",
        "            if input_shape[0] not in {1, 3}:\n",
        "                warnings.warn(\n",
        "                    'This model usually expects 1 or 3 input channels. '\n",
        "                    'However, it was passed an input_shape with ' +\n",
        "                    str(input_shape[0]) + ' input channels.')\n",
        "            default_shape = (input_shape[0], default_size, default_size)\n",
        "        else:\n",
        "            if input_shape[-1] not in {1, 3}:\n",
        "                warnings.warn(\n",
        "                    'This model usually expects 1 or 3 input channels. '\n",
        "                    'However, it was passed an input_shape with ' +\n",
        "                    str(input_shape[-1]) + ' input channels.')\n",
        "            default_shape = (default_size, default_size, input_shape[-1])\n",
        "    else:\n",
        "        if data_format == 'channels_first':\n",
        "            default_shape = (3, default_size, default_size)\n",
        "        else:\n",
        "            default_shape = (default_size, default_size, 3)\n",
        "    if weights == 'imagenet' and require_flatten:\n",
        "        if input_shape is not None:\n",
        "            if input_shape != default_shape:\n",
        "                raise ValueError('When setting `include_top=True` '\n",
        "                                 'and loading `imagenet` weights, '\n",
        "                                 '`input_shape` should be ' +\n",
        "                                 str(default_shape) + '.')\n",
        "        return default_shape\n",
        "    if input_shape:\n",
        "        if data_format == 'channels_first':\n",
        "            if input_shape is not None:\n",
        "                if len(input_shape) != 3:\n",
        "                    raise ValueError(\n",
        "                        '`input_shape` must be a tuple of three integers.')\n",
        "                if input_shape[0] != 3 and weights == 'imagenet':\n",
        "                    raise ValueError('The input must have 3 channels; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "                if ((input_shape[1] is not None and input_shape[1] < min_size) or\n",
        "                   (input_shape[2] is not None and input_shape[2] < min_size)):\n",
        "                    raise ValueError('Input size must be at least ' +\n",
        "                                     str(min_size) + 'x' + str(min_size) +\n",
        "                                     '; got `input_shape=' +\n",
        "                                     str(input_shape) + '`')\n",
        "        else:\n",
        "            if input_shape is not None:\n",
        "                if len(input_shape) != 3:\n",
        "                    raise ValueError(\n",
        "                        '`input_shape` must be a tuple of three integers.')\n",
        "                if input_shape[-1] != 3 and weights == 'imagenet':\n",
        "                    raise ValueError('The input must have 3 channels; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "                if ((input_shape[0] is not None and input_shape[0] < min_size) or\n",
        "                   (input_shape[1] is not None and input_shape[1] < min_size)):\n",
        "                    raise ValueError('Input size must be at least ' +\n",
        "                                     str(min_size) + 'x' + str(min_size) +\n",
        "                                     '; got `input_shape=' +\n",
        "                                     str(input_shape) + '`')\n",
        "    else:\n",
        "        if require_flatten:\n",
        "            input_shape = default_shape\n",
        "        else:\n",
        "            if data_format == 'channels_first':\n",
        "                input_shape = (3, None, None)\n",
        "            else:\n",
        "                input_shape = (None, None, 3)\n",
        "    if require_flatten:\n",
        "        if None in input_shape:\n",
        "            raise ValueError('If `include_top` is True, '\n",
        "                             'you should specify a static `input_shape`. '\n",
        "                             'Got `input_shape=' + str(input_shape) + '`')\n",
        "    return input_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRdHApE0Dmrp",
        "colab_type": "text"
      },
      "source": [
        "### Generate model. We need to compile this model and eventually train for one loop to workaround what may be a possible bug around weights initialization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDW3Wq0GKYBh",
        "colab_type": "code",
        "outputId": "b4f11ab4-15a7-446b-f5d6-e32ec66e570c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2268
        }
      },
      "source": [
        "# from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from cleverhans.utils_keras import KerasModelWrapper\n",
        "\n",
        "def get_mnv2(alpha=.35):\n",
        "  # All parameters are default except weights = None because we will be training this ourselves on cifar10, not ImageNet, classes=10. From the docs:\n",
        "  # https://keras.io/applications/#mobilenetv2\n",
        "  \n",
        "  # x = tensorflow.keras.layers.Input(shape=(96,96,3))\n",
        "  model = MobileNetV2(input_shape=(32,32,3), alpha=alpha, include_top=True, weights=None, input_tensor=None, pooling=None, classes=10)\n",
        "\n",
        "  opt = tensorflow.keras.optimizers.RMSprop(learning_rate=0.001, rho=1)\n",
        "\n",
        "  model.compile(optimizer='RMSProp',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  # for layer in model.layers:\n",
        "  # print(layer.trainable)\n",
        "\n",
        "  # x = tensorflow.keras.layers.Input(shape=(32,32,3))\n",
        "  # y = tensorflow.keras.layers.Flatten()(x)\n",
        "  # y = tensorflow.keras.layers.Dense(10, activation='softmax')(y)\n",
        "  \n",
        "  # model = tensorflow.keras.models.Model(inputs=x, outputs=y)\n",
        "  return model\n",
        "\n",
        "model = get_mnv2()\n",
        "print(model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"mobilenetv2_0.35_32\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Conv1_pad (ZeroPadding2D)       (None, 33, 33, 3)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Conv1 (Conv2D)                  (None, 16, 16, 16)   432         Conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bn_Conv1 (BatchNormalization)   (None, 16, 16, 16)   64          Conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Conv1_relu (ReLU)               (None, 16, 16, 16)   0           bn_Conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise (Depthw (None, 16, 16, 16)   144         Conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise_BN (Bat (None, 16, 16, 16)   64          expanded_conv_depthwise[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise_relu (R (None, 16, 16, 16)   0           expanded_conv_depthwise_BN[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_project (Conv2D)  (None, 16, 16, 8)    128         expanded_conv_depthwise_relu[0][0\n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_project_BN (Batch (None, 16, 16, 8)    32          expanded_conv_project[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand (Conv2D)         (None, 16, 16, 48)   384         expanded_conv_project_BN[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand_BN (BatchNormali (None, 16, 16, 48)   192         block_1_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand_relu (ReLU)      (None, 16, 16, 48)   0           block_1_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_pad (ZeroPadding2D)     (None, 17, 17, 48)   0           block_1_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise (DepthwiseCon (None, 8, 8, 48)     432         block_1_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise_BN (BatchNorm (None, 8, 8, 48)     192         block_1_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise_relu (ReLU)   (None, 8, 8, 48)     0           block_1_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_1_project (Conv2D)        (None, 8, 8, 8)      384         block_1_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_1_project_BN (BatchNormal (None, 8, 8, 8)      32          block_1_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand (Conv2D)         (None, 8, 8, 48)     384         block_1_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand_BN (BatchNormali (None, 8, 8, 48)     192         block_2_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand_relu (ReLU)      (None, 8, 8, 48)     0           block_2_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise (DepthwiseCon (None, 8, 8, 48)     432         block_2_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise_BN (BatchNorm (None, 8, 8, 48)     192         block_2_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise_relu (ReLU)   (None, 8, 8, 48)     0           block_2_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_2_project (Conv2D)        (None, 8, 8, 8)      384         block_2_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_2_project_BN (BatchNormal (None, 8, 8, 8)      32          block_2_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_2_add (Add)               (None, 8, 8, 8)      0           block_1_project_BN[0][0]         \n",
            "                                                                 block_2_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand (Conv2D)         (None, 8, 8, 48)     384         block_2_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand_BN (BatchNormali (None, 8, 8, 48)     192         block_3_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand_relu (ReLU)      (None, 8, 8, 48)     0           block_3_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_pad (ZeroPadding2D)     (None, 9, 9, 48)     0           block_3_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise (DepthwiseCon (None, 4, 4, 48)     432         block_3_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise_BN (BatchNorm (None, 4, 4, 48)     192         block_3_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise_relu (ReLU)   (None, 4, 4, 48)     0           block_3_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_3_project (Conv2D)        (None, 4, 4, 16)     768         block_3_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_3_project_BN (BatchNormal (None, 4, 4, 16)     64          block_3_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand (Conv2D)         (None, 4, 4, 96)     1536        block_3_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand_BN (BatchNormali (None, 4, 4, 96)     384         block_4_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand_relu (ReLU)      (None, 4, 4, 96)     0           block_4_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise (DepthwiseCon (None, 4, 4, 96)     864         block_4_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise_BN (BatchNorm (None, 4, 4, 96)     384         block_4_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise_relu (ReLU)   (None, 4, 4, 96)     0           block_4_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_4_project (Conv2D)        (None, 4, 4, 16)     1536        block_4_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_4_project_BN (BatchNormal (None, 4, 4, 16)     64          block_4_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_4_add (Add)               (None, 4, 4, 16)     0           block_3_project_BN[0][0]         \n",
            "                                                                 block_4_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand (Conv2D)         (None, 4, 4, 96)     1536        block_4_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand_BN (BatchNormali (None, 4, 4, 96)     384         block_5_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand_relu (ReLU)      (None, 4, 4, 96)     0           block_5_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise (DepthwiseCon (None, 4, 4, 96)     864         block_5_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise_BN (BatchNorm (None, 4, 4, 96)     384         block_5_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise_relu (ReLU)   (None, 4, 4, 96)     0           block_5_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_5_project (Conv2D)        (None, 4, 4, 16)     1536        block_5_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_5_project_BN (BatchNormal (None, 4, 4, 16)     64          block_5_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_5_add (Add)               (None, 4, 4, 16)     0           block_4_add[0][0]                \n",
            "                                                                 block_5_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Conv_1 (Conv2D)                 (None, 4, 4, 1280)   20480       block_5_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Conv_1_bn (BatchNormalization)  (None, 4, 4, 1280)   5120        Conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "out_relu (ReLU)                 (None, 4, 4, 1280)   0           Conv_1_bn[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 1280)         0           out_relu[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Logits (Dense)                  (None, 10)           12810       global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 54,074\n",
            "Trainable params: 49,962\n",
            "Non-trainable params: 4,112\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bevLmhN3yH-",
        "colab_type": "text"
      },
      "source": [
        "### Train mobilenetv2, run pgd on it. \n",
        "\n",
        "This is mostly from the Cleverhans tutorial on Keras models. \n",
        "\n",
        "There is one unexpected trick here. The model needs to be trained in one loop of Keras training before handing it over to the Cleverhans training loop. It may be that weigths are not initialized properly, or, an initial run may be needed to get the TF variables listed out properly. \n",
        "\n",
        "The truncated MobileNetV2 model with these stats:\n",
        "\n",
        "<pre>\n",
        "Total params: 54,074\n",
        "Trainable params: 49,962\n",
        "Non-trainable params: 4,112\n",
        "</pre>\n",
        "\n",
        "Reaches 65% top-1 accuracy on CIFAR-10 after about 15 epochs. \n",
        "\n",
        "<pre>\n",
        "[INFO 2020-01-21 22:06:11,473 cleverhans] Epoch 14 took 11.071139812469482 seconds\n",
        "Test accuracy on legitimate examples: 0.644500\n",
        "[INFO 2020-01-21 22:06:23,392 cleverhans] Epoch 15 took 11.085895776748657 seconds\n",
        "Test accuracy on legitimate examples: 0.646700\n",
        "[INFO 2020-01-21 22:06:35,024 cleverhans] Epoch 16 took 10.808518648147583 seconds\n",
        "Test accuracy on legitimate examples: 0.653600\n",
        "[INFO 2020-01-21 22:06:47,575 cleverhans] Epoch 17 took 11.663419008255005 seconds\n",
        "Test accuracy on legitimate examples: 0.646000\n",
        "[INFO 2020-01-21 22:06:59,908 cleverhans] Epoch 18 took 11.490140199661255 seconds\n",
        "Test accuracy on legitimate examples: 0.643300\n",
        "[INFO 2020-01-21 22:07:11,687 cleverhans] Epoch 19 took 10.94323444366455 seconds\n",
        "Test accuracy on legitimate examples: 0.658200\n",
        "</pre>\n",
        "\n",
        "Adversarial retraining with Madry e8 increases both clean and adversarial accuracy.\n",
        "\n",
        "<pre>\n",
        "[INFO 2020-01-22 01:08:49,993 cleverhans] Epoch 0 took 112.98240733146667 seconds\n",
        "Test accuracy on legitimate examples: 0.4187\n",
        "Test accuracy on adversarial examples: 0.2815\n",
        "[INFO 2020-01-22 01:11:07,334 cleverhans] Epoch 1 took 110.0533185005188 seconds\n",
        "Test accuracy on legitimate examples: 0.4538\n",
        "Test accuracy on adversarial examples: 0.3147\n",
        "...\n",
        "[INFO 2020-01-22 01:47:44,696 cleverhans] Epoch 18 took 109.45361185073853 seconds\n",
        "Test accuracy on legitimate examples: 0.5932\n",
        "Test accuracy on adversarial examples: 0.3619\n",
        "[INFO 2020-01-22 01:49:53,164 cleverhans] Epoch 19 took 109.46263790130615 seconds\n",
        "Test accuracy on legitimate examples: 0.6000\n",
        "Test accuracy on adversarial examples: 0.3624\n",
        "<cleverhans.utils.AccuracyReport at 0x7f5ec5400eb8>\n",
        "</pre>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pszj9OnX20C2",
        "colab_type": "code",
        "outputId": "6c12f252-f64a-45a0-d9db-a19b49784339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2086
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# from cleverhans.attacks import FastGradientMethod\n",
        "from cleverhans.attacks import MadryEtAl\n",
        "from cleverhans.compat import flags\n",
        "# from cleverhans.dataset import MNIST\n",
        "from cleverhans.dataset import CIFAR10\n",
        "from cleverhans.loss import CrossEntropy\n",
        "from cleverhans.train import train\n",
        "from cleverhans.utils import AccuracyReport\n",
        "from cleverhans.utils_keras import cnn_model\n",
        "from cleverhans.utils_keras import KerasModelWrapper\n",
        "from cleverhans.utils_tf import model_eval\n",
        "\n",
        "def cifar_pgd(     train_start=   0, \n",
        "                   train_end=     50000, \n",
        "                   test_start=    0,\n",
        "                   test_end=      10000, \n",
        "                   nb_epochs=     20,\n",
        "                   batch_size=    64,\n",
        "                   learning_rate= 0.001, \n",
        "                   train_dir=     \".\",\n",
        "                   filename=      \"./cleverhansout\",\n",
        "                   load_model=    False,\n",
        "                   testing=       True, \n",
        "                   label_smoothing=0.0):\n",
        "  \"\"\"\n",
        "  Derived from MNIST CleverHans tutorial\n",
        "\n",
        "  Adjusted to use Keras MobileNet V2 model, CIFAR, and PGD.\n",
        "\n",
        "  :param train_start: index of first training set example\n",
        "  :param train_end: index of last training set example\n",
        "  :param test_start: index of first test set example\n",
        "  :param test_end: index of last test set example\n",
        "  :param nb_epochs: number of epochs to train model\n",
        "  :param batch_size: size of training batches\n",
        "  :param learning_rate: learning rate for training\n",
        "  :param train_dir: Directory storing the saved model\n",
        "  :param filename: Filename to save model under\n",
        "  :param load_model: True for load, False for not load\n",
        "  :param testing: if true, test error is calculated\n",
        "  :param label_smoothing: float, amount of label smoothing for cross entropy\n",
        "  :return: an AccuracyReport object\n",
        "  \"\"\"\n",
        "  tf.keras.backend.set_learning_phase(0)\n",
        "\n",
        "  # Object used to keep track of (and return) key accuracies\n",
        "  report = AccuracyReport()\n",
        "\n",
        "  # Set TF random seed to improve reproducibility\n",
        "  tf.set_random_seed(1234)\n",
        "\n",
        "  if keras.backend.image_data_format() != 'channels_last':\n",
        "    raise NotImplementedError(\"this tutorial requires keras to be configured to channels_last format\")\n",
        "\n",
        "  # Create TF session and set as Keras backend session\n",
        "  with tf.Session() as sess:\n",
        "\n",
        "  # sess = tf.Session()\n",
        "    keras.backend.set_session(sess)\n",
        "\n",
        "    # Get test data\n",
        "    cifar = CIFAR10(train_start=train_start, train_end=train_end,\n",
        "                  test_start=test_start, test_end=test_end)\n",
        "    x_train, y_train = cifar.get_set('train')\n",
        "    x_test, y_test = cifar.get_set('test')\n",
        "\n",
        "    # Obtain Image Parameters\n",
        "    img_rows, img_cols, nchannels = x_train.shape[1:4]\n",
        "    nb_classes = y_train.shape[1]\n",
        "\n",
        "    # Define input TF placeholder\n",
        "    x = tf.placeholder(tf.float32, shape=(None, img_rows, img_cols,\n",
        "                                          nchannels))\n",
        "    y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
        "\n",
        "    # Define TF model graph\n",
        "    # model = cnn_model(img_rows=img_rows, img_cols=img_cols,\n",
        "    #                   channels=nchannels, nb_filters=64,\n",
        "    #                   nb_classes=nb_classes)\n",
        "    model = get_mnv2()\n",
        "    preds = model(x)\n",
        "    print(\"Defined TensorFlow model graph.\")\n",
        "\n",
        "    def evaluate():\n",
        "      # Evaluate the accuracy of the model on legitimate test examples\n",
        "      eval_params = {'batch_size': batch_size}\n",
        "      acc = model_eval(sess, x, y, preds, x_test, y_test, args=eval_params)\n",
        "      report.clean_train_clean_eval = acc\n",
        "    #        assert X_test.shape[0] == test_end - test_start, X_test.shape\n",
        "      print('Test accuracy on legitimate examples: %0.6f' % acc)\n",
        "\n",
        "    # Train model\n",
        "    train_params = {\n",
        "        'nb_epochs': nb_epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'learning_rate': learning_rate,\n",
        "        'train_dir': train_dir,\n",
        "        'filename': filename\n",
        "    }\n",
        "\n",
        "    rng = np.random.RandomState([2017, 8, 30])\n",
        "    if not os.path.exists(train_dir):\n",
        "      os.mkdir(train_dir)\n",
        "\n",
        "    ckpt = tf.train.get_checkpoint_state(train_dir)\n",
        "    print(train_dir, ckpt)\n",
        "    ckpt_path = False if ckpt is None else ckpt.model_checkpoint_path\n",
        "    wrap = KerasModelWrapper(model)\n",
        "\n",
        "    if load_model and ckpt_path:\n",
        "      saver = tf.train.Saver()\n",
        "      print(ckpt_path)\n",
        "      saver.restore(sess, ckpt_path)\n",
        "      print(\"Model loaded from: {}\".format(ckpt_path))\n",
        "      evaluate()\n",
        "    else:\n",
        "      print(\"Model was not loaded, training from scratch.\")\n",
        "      loss = CrossEntropy(wrap, smoothing=label_smoothing)\n",
        "\n",
        "      # Do one cycle of Keras training. This may be to work around a \n",
        "      # possible bug in weight non-initialization.\n",
        "      #\n",
        "      model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=1, verbose=1, \n",
        "                callbacks=None, validation_split=0.0, validation_data=None, \n",
        "                shuffle=True, class_weight=None, sample_weight=None, \n",
        "                initial_epoch=0, steps_per_epoch=None, validation_steps=None, \n",
        "                validation_freq=1, max_queue_size=10, workers=1, \n",
        "                use_multiprocessing=False)\n",
        "\n",
        "      rms = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "      train(sess, loss, x_train, y_train, evaluate=evaluate,\n",
        "            # optimizer=rms,\n",
        "            args=train_params, rng=rng)\n",
        "\n",
        "    # Calculate training error\n",
        "    if testing:\n",
        "      eval_params = {'batch_size': batch_size}\n",
        "      acc = model_eval(sess, x, y, preds, x_train, y_train, args=eval_params)\n",
        "      report.train_clean_train_clean_eval = acc\n",
        "\n",
        "    # Initialize the attack object and graph\n",
        "    madry = MadryEtAl(wrap, sess=sess)\n",
        "    # TODO: Check to see that these params are application to Madry. These were lifted from FGSM.\n",
        "    madry_params = {'eps': 8./255., # 0.3,\n",
        "                    'eps_iter': 2./255.,\n",
        "                    'clip_min': 0.,\n",
        "                    'clip_max': 1.}\n",
        "    adv_x = madry.generate(x, **madry_params)\n",
        "    # Consider the attack to be constant\n",
        "    adv_x = tf.stop_gradient(adv_x)\n",
        "    preds_adv = model(adv_x)\n",
        "\n",
        "    # Evaluate the accuracy of the MNIST model on adversarial examples\n",
        "    eval_par = {'batch_size': batch_size}\n",
        "    acc = model_eval(sess, x, y, preds_adv, x_test, y_test, args=eval_par)\n",
        "    print('Test accuracy on adversarial examples: %0.6f\\n' % acc)\n",
        "    report.clean_train_adv_eval = acc\n",
        "\n",
        "    # Calculating train error\n",
        "    if testing:\n",
        "      eval_par = {'batch_size': batch_size}\n",
        "      acc = model_eval(sess, x, y, preds_adv, x_train,\n",
        "                        y_train, args=eval_par)\n",
        "      report.train_clean_train_adv_eval = acc\n",
        "\n",
        "    print(\"Repeating the process, using adversarial training\")\n",
        "    # Redefine TF model graph\n",
        "    # Ensue that the model is new. \n",
        "    model_2 = get_mnv2()\n",
        "    wrap_2 = KerasModelWrapper(model_2)\n",
        "    preds_2 = model_2(x)\n",
        "    madry2 = MadryEtAl(wrap_2, sess=sess)\n",
        "\n",
        "    model_2.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=1, verbose=1, \n",
        "              callbacks=None, validation_split=0.0, validation_data=None, \n",
        "              shuffle=True, class_weight=None, sample_weight=None, \n",
        "              initial_epoch=0, steps_per_epoch=None, validation_steps=None, \n",
        "              validation_freq=1, max_queue_size=10, workers=1, \n",
        "              use_multiprocessing=False)\n",
        "\n",
        "    def attack(x):\n",
        "      return madry2.generate(x, **madry_params)\n",
        "\n",
        "    preds_2_adv = model_2(attack(x))\n",
        "    loss_2 = CrossEntropy(wrap_2, smoothing=label_smoothing, attack=attack)\n",
        "\n",
        "    def evaluate_2():\n",
        "      # Accuracy of adversarially trained model on legitimate test inputs\n",
        "      eval_params = {'batch_size': batch_size}\n",
        "      accuracy = model_eval(sess, x, y, preds_2, x_test, y_test,\n",
        "                            args=eval_params)\n",
        "      print('Test accuracy on legitimate examples: %0.4f' % accuracy)\n",
        "      report.adv_train_clean_eval = accuracy\n",
        "\n",
        "      # Accuracy of the adversarially trained model on adversarial examples\n",
        "      accuracy = model_eval(sess, x, y, preds_2_adv, x_test,\n",
        "                            y_test, args=eval_params)\n",
        "      print('Test accuracy on adversarial examples: %0.4f' % accuracy)\n",
        "      report.adv_train_adv_eval = accuracy\n",
        "\n",
        "    # Perform and evaluate adversarial training\n",
        "    train(sess, loss_2, x_train, y_train, evaluate=evaluate_2,\n",
        "          args=train_params, rng=rng)\n",
        "\n",
        "    # Calculate training errors\n",
        "    if testing:\n",
        "      eval_params = {'batch_size': batch_size}\n",
        "      accuracy = model_eval(sess, x, y, preds_2, x_train, y_train,\n",
        "                            args=eval_params)\n",
        "      report.train_adv_train_clean_eval = accuracy\n",
        "      accuracy = model_eval(sess, x, y, preds_2_adv, x_train,\n",
        "                            y_train, args=eval_params)\n",
        "      report.train_adv_train_adv_eval = accuracy\n",
        "    # end with tf.Session():\n",
        "\n",
        "  return report\n",
        "\n",
        "cifar_pgd(batch_size=32, nb_epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "Defined TensorFlow model graph.\n",
            ". None\n",
            "Model was not loaded, training from scratch.\n",
            "Train on 50000 samples\n",
            "50000/50000 [==============================] - 21s 426us/sample - loss: 2.1751 - acc: 0.1656\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:00:06,494 cleverhans] Epoch 0 took 13.329843997955322 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.409200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:00:22,064 cleverhans] Epoch 1 took 11.162869930267334 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.436500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:00:34,220 cleverhans] Epoch 2 took 11.305679559707642 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.504700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:00:46,403 cleverhans] Epoch 3 took 11.287597894668579 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.521500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:00:59,029 cleverhans] Epoch 4 took 11.679455757141113 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.551100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:01:11,470 cleverhans] Epoch 5 took 11.579076051712036 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.566100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:01:23,558 cleverhans] Epoch 6 took 11.23734450340271 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.570100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:01:35,985 cleverhans] Epoch 7 took 11.573017358779907 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.583000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:01:48,313 cleverhans] Epoch 8 took 11.442841053009033 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.600100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:02:00,694 cleverhans] Epoch 9 took 11.335247993469238 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.604100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:02:12,878 cleverhans] Epoch 10 took 11.326795816421509 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.610800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:02:25,159 cleverhans] Epoch 11 took 11.423813104629517 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.617900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:02:37,542 cleverhans] Epoch 12 took 11.50550389289856 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.627000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:02:50,778 cleverhans] Epoch 13 took 12.32450246810913 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.623100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:03:03,126 cleverhans] Epoch 14 took 11.471078395843506 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.639000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:03:15,640 cleverhans] Epoch 15 took 11.653642654418945 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.640100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:03:27,849 cleverhans] Epoch 16 took 11.315950632095337 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.643800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:03:40,263 cleverhans] Epoch 17 took 11.558930158615112 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.648400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:03:52,851 cleverhans] Epoch 18 took 11.73654317855835 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.650400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:04:05,492 cleverhans] Epoch 19 took 11.744518280029297 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.655100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/src/cleverhans/cleverhans/compat.py:22: UserWarning: <function reduce_sum_v1 at 0x7f5f9e038620> is deprecated. Switch to calling the equivalent function in tensorflow.  This function was originally needed as a compatibility layer for old versions of tensorflow,  but support for those versions has now been dropped.\n",
            "  warnings.warn(str(f) + \" is deprecated. Switch to calling the equivalent function in tensorflow. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on adversarial examples: 0.122500\n",
            "\n",
            "Repeating the process, using adversarial training\n",
            "Train on 50000 samples\n",
            "50000/50000 [==============================] - 22s 439us/sample - loss: 2.0243 - acc: 0.2242\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/src/cleverhans/cleverhans/compat.py:22: UserWarning: <function reduce_sum_v1 at 0x7f5f9e038620> is deprecated. Switch to calling the equivalent function in tensorflow.  This function was originally needed as a compatibility layer for old versions of tensorflow,  but support for those versions has now been dropped.\n",
            "  warnings.warn(str(f) + \" is deprecated. Switch to calling the equivalent function in tensorflow. \"\n",
            "/content/src/cleverhans/cleverhans/loss.py:41: UserWarning: callable attacks are deprecated, switch to an Attack subclass. callable attacks will not be supported after 2019-05-05.\n",
            "  warnings.warn(\"callable attacks are deprecated, switch to an Attack \"\n",
            "/content/src/cleverhans/cleverhans/compat.py:22: UserWarning: <function reduce_sum_v1 at 0x7f5f9e038620> is deprecated. Switch to calling the equivalent function in tensorflow.  This function was originally needed as a compatibility layer for old versions of tensorflow,  but support for those versions has now been dropped.\n",
            "  warnings.warn(str(f) + \" is deprecated. Switch to calling the equivalent function in tensorflow. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:08:49,993 cleverhans] Epoch 0 took 112.98240733146667 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.4187\n",
            "Test accuracy on adversarial examples: 0.2815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:11:07,334 cleverhans] Epoch 1 took 110.0533185005188 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.4538\n",
            "Test accuracy on adversarial examples: 0.3147\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:13:16,218 cleverhans] Epoch 2 took 110.19014978408813 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.4923\n",
            "Test accuracy on adversarial examples: 0.3319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:15:26,400 cleverhans] Epoch 3 took 111.28938484191895 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5096\n",
            "Test accuracy on adversarial examples: 0.3456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:17:39,408 cleverhans] Epoch 4 took 113.27765417098999 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5219\n",
            "Test accuracy on adversarial examples: 0.3402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:19:50,991 cleverhans] Epoch 5 took 111.40648746490479 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5369\n",
            "Test accuracy on adversarial examples: 0.3417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:22:00,382 cleverhans] Epoch 6 took 110.04383587837219 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5235\n",
            "Test accuracy on adversarial examples: 0.3455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:24:09,576 cleverhans] Epoch 7 took 110.36229038238525 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5437\n",
            "Test accuracy on adversarial examples: 0.3628\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:26:17,250 cleverhans] Epoch 8 took 109.18047666549683 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5330\n",
            "Test accuracy on adversarial examples: 0.3532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:28:27,253 cleverhans] Epoch 9 took 110.86108469963074 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5364\n",
            "Test accuracy on adversarial examples: 0.3605\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:30:36,888 cleverhans] Epoch 10 took 109.8416965007782 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5592\n",
            "Test accuracy on adversarial examples: 0.3617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:32:45,251 cleverhans] Epoch 11 took 109.24990940093994 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5813\n",
            "Test accuracy on adversarial examples: 0.3538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:34:53,952 cleverhans] Epoch 12 took 109.58411264419556 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5782\n",
            "Test accuracy on adversarial examples: 0.3481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:37:01,865 cleverhans] Epoch 13 took 109.00561594963074 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5888\n",
            "Test accuracy on adversarial examples: 0.3629\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:39:10,675 cleverhans] Epoch 14 took 110.08999133110046 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5782\n",
            "Test accuracy on adversarial examples: 0.3642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:41:18,472 cleverhans] Epoch 15 took 109.10942268371582 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5840\n",
            "Test accuracy on adversarial examples: 0.3563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:43:26,587 cleverhans] Epoch 16 took 108.9611828327179 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5950\n",
            "Test accuracy on adversarial examples: 0.3529\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:45:36,246 cleverhans] Epoch 17 took 110.32626533508301 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5838\n",
            "Test accuracy on adversarial examples: 0.3578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:47:44,696 cleverhans] Epoch 18 took 109.45361185073853 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5932\n",
            "Test accuracy on adversarial examples: 0.3619\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 01:49:53,164 cleverhans] Epoch 19 took 109.46263790130615 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6000\n",
            "Test accuracy on adversarial examples: 0.3624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cleverhans.utils.AccuracyReport at 0x7f5ec5400eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKDjR1LkPJlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}