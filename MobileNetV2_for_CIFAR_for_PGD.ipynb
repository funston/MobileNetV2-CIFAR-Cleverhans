{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNetV2 for CIFAR for PGD",
      "provenance": [],
      "collapsed_sections": [
        "YX3ojIoKCQa1",
        "K4mwCDxR3otR",
        "O0zBGmj4BIE4"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaoshiang/MobileNetV2-CIFAR-Cleverhans/blob/master/MobileNetV2_for_CIFAR_for_PGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fus01p5tCEmG",
        "colab_type": "text"
      },
      "source": [
        "# Applying Cleverhans to a version of MobileNetV2.\n",
        "\n",
        "We specifically choose a modern architecture to analyze. A CNN built solely on convolutions and pooling are by now far out of date. We wanted a CNN with batchnorm, bottlenecks, and residual blocks. \n",
        "\n",
        "But training on ImageNet is expensive and slow. So we decided build for CIFAR10. This required us to shorten MobileNetV2 to prevent overstriding down of the feature maps down to 1x1 degenerate maps. \n",
        "\n",
        "https://arxiv.org/abs/1911.09665\n",
        "\n",
        "How Does Batch Normalization Help Optimization?\n",
        "https://arxiv.org/pdf/1805.11604.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX3ojIoKCQa1",
        "colab_type": "text"
      },
      "source": [
        "### Setup libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ThssnZ0KLV-",
        "colab_type": "code",
        "outputId": "651eb0c2-c288-4581-d4e3-606eac59a73e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "%tensorflow_version 1.14\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `1.14`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RTVj-bR6Udn",
        "colab_type": "code",
        "outputId": "6ce43fef-78b9-4cb0-cbf7-91ae24e15742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import keras\n",
        "# print(keras.__version__)\n",
        "print(tensorflow.keras.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySXDM4FB_Wy8",
        "colab_type": "code",
        "outputId": "d45a50a8-03cc-4a4e-fe24-0c35784ed74f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 1775142776655380468\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 17196867607156065076\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 13850506392617947761\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14912199066\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 17836479255619387926\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erBmVTtG0iHq",
        "colab_type": "text"
      },
      "source": [
        "The version of cleverhans forked at github.com/yaoshiang/cleverhans is v3.0.1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIoZLWj6JQPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "add51b23-3d4c-41b3-d4b2-4bca5f1cc0a3"
      },
      "source": [
        "!pip install -qq -e git+http://github.com/yaoshiang/cleverhans.git#egg=cleverhans\n",
        "import sys\n",
        "sys.path.append('/content/src/cleverhans')\n",
        "import cleverhans"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 163kB 2.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 3.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGtv7kFh_dRJ",
        "colab_type": "code",
        "outputId": "79dfdf9f-b5d1-46d5-a20e-ec374ed22777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jan 22 17:45:46 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    30W /  70W |    111MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgT1GvjkN6_z",
        "colab_type": "code",
        "outputId": "b9aa700a-0d80-41de-81ae-f78ee565bb08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import cleverhans_tutorials as ct\n",
        "import cleverhans_tutorials.cifar10_tutorial_tf as ctc"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/src/cleverhans/cleverhans/utils_tf.py:345: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zfn0DEyyrxN",
        "colab_type": "text"
      },
      "source": [
        "### Create a cleverhans wrapper for the Keras version of MobileNet v2.\n",
        "\n",
        "We use MobileNet V2 as an example of \"modern\" image classifier architecture, which includes Residual Blocks and batchnorms. The next generation after that such as MobileNetV3 are generally discovered via AutoML searches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwAgrBlXHWuh",
        "colab_type": "text"
      },
      "source": [
        "### First lift out of a lot of Keras source code and modify the Keras implementation of MobileNetV2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGFZ2MxT2tsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correct_pad(backend, inputs, kernel_size):\n",
        "    \"\"\"Returns a tuple for zero-padding for 2D convolution with downsampling.\n",
        "    # Arguments\n",
        "        input_size: An integer or tuple/list of 2 integers.\n",
        "        kernel_size: An integer or tuple/list of 2 integers.\n",
        "    # Returns\n",
        "        A tuple.\n",
        "    \"\"\"\n",
        "    img_dim = 2 if backend.image_data_format() == 'channels_first' else 1\n",
        "    input_size = backend.int_shape(inputs)[img_dim:(img_dim + 2)]\n",
        "\n",
        "    if isinstance(kernel_size, int):\n",
        "        kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "    if input_size[0] is None:\n",
        "        adjust = (1, 1)\n",
        "    else:\n",
        "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
        "\n",
        "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
        "\n",
        "    return ((correct[0] - adjust[0], correct[0]),\n",
        "            (correct[1] - adjust[1], correct[1]))\n",
        "    \n",
        "\n",
        "    \"\"\"MobileNet v2 models for Keras.\n",
        "MobileNetV2 is a general architecture and can be used for multiple use cases.\n",
        "Depending on the use case, it can use different input layer size and\n",
        "different width factors. This allows different width models to reduce\n",
        "the number of multiply-adds and thereby\n",
        "reduce inference cost on mobile devices.\n",
        "MobileNetV2 is very similar to the original MobileNet,\n",
        "except that it uses inverted residual blocks with\n",
        "bottlenecking features. It has a drastically lower\n",
        "parameter count than the original MobileNet.\n",
        "MobileNets support any input size greater\n",
        "than 32 x 32, with larger image sizes\n",
        "offering better performance.\n",
        "The number of parameters and number of multiply-adds\n",
        "can be modified by using the `alpha` parameter,\n",
        "which increases/decreases the number of filters in each layer.\n",
        "By altering the image size and `alpha` parameter,\n",
        "all 22 models from the paper can be built, with ImageNet weights provided.\n",
        "The paper demonstrates the performance of MobileNets using `alpha` values of\n",
        "1.0 (also called 100 % MobileNet), 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4\n",
        "For each of these `alpha` values, weights for 5 different input image sizes\n",
        "are provided (224, 192, 160, 128, and 96).\n",
        "The following table describes the performance of\n",
        "MobileNet on various input sizes:\n",
        "------------------------------------------------------------------------\n",
        "MACs stands for Multiply Adds\n",
        " Classification Checkpoint| MACs (M) | Parameters (M)| Top 1 Accuracy| Top 5 Accuracy\n",
        "--------------------------|------------|---------------|---------|----|-------------\n",
        "| [mobilenet_v2_1.4_224]  | 582 | 6.06 |          75.0 | 92.5 |\n",
        "| [mobilenet_v2_1.3_224]  | 509 | 5.34 |          74.4 | 92.1 |\n",
        "| [mobilenet_v2_1.0_224]  | 300 | 3.47 |          71.8 | 91.0 |\n",
        "| [mobilenet_v2_1.0_192]  | 221 | 3.47 |          70.7 | 90.1 |\n",
        "| [mobilenet_v2_1.0_160]  | 154 | 3.47 |          68.8 | 89.0 |\n",
        "| [mobilenet_v2_1.0_128]  | 99  | 3.47 |          65.3 | 86.9 |\n",
        "| [mobilenet_v2_1.0_96]   | 56  | 3.47 |          60.3 | 83.2 |\n",
        "| [mobilenet_v2_0.75_224] | 209 | 2.61 |          69.8 | 89.6 |\n",
        "| [mobilenet_v2_0.75_192] | 153 | 2.61 |          68.7 | 88.9 |\n",
        "| [mobilenet_v2_0.75_160] | 107 | 2.61 |          66.4 | 87.3 |\n",
        "| [mobilenet_v2_0.75_128] | 69  | 2.61 |          63.2 | 85.3 |\n",
        "| [mobilenet_v2_0.75_96]  | 39  | 2.61 |          58.8 | 81.6 |\n",
        "| [mobilenet_v2_0.5_224]  | 97  | 1.95 |          65.4 | 86.4 |\n",
        "| [mobilenet_v2_0.5_192]  | 71  | 1.95 |          63.9 | 85.4 |\n",
        "| [mobilenet_v2_0.5_160]  | 50  | 1.95 |          61.0 | 83.2 |\n",
        "| [mobilenet_v2_0.5_128]  | 32  | 1.95 |          57.7 | 80.8 |\n",
        "| [mobilenet_v2_0.5_96]   | 18  | 1.95 |          51.2 | 75.8 |\n",
        "| [mobilenet_v2_0.35_224] | 59  | 1.66 |          60.3 | 82.9 |\n",
        "| [mobilenet_v2_0.35_192] | 43  | 1.66 |          58.2 | 81.2 |\n",
        "| [mobilenet_v2_0.35_160] | 30  | 1.66 |          55.7 | 79.1 |\n",
        "| [mobilenet_v2_0.35_128] | 20  | 1.66 |          50.8 | 75.0 |\n",
        "| [mobilenet_v2_0.35_96]  | 11  | 1.66 |          45.5 | 70.4 |\n",
        "The weights for all 16 models are obtained and\n",
        "translated from the Tensorflow checkpoints\n",
        "from TensorFlow checkpoints found [here]\n",
        "(https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md).\n",
        "# Reference\n",
        "This file contains building code for MobileNetV2, based on\n",
        "[MobileNetV2: Inverted Residuals and Linear Bottlenecks]\n",
        "(https://arxiv.org/abs/1801.04381) (CVPR 2018)\n",
        "Tests comparing this model to the existing Tensorflow model can be\n",
        "found at [mobilenet_v2_keras]\n",
        "(https://github.com/JonathanCMitchell/mobilenet_v2_keras)\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "# TODO Change path to v1.1\n",
        "BASE_WEIGHT_PATH = ('https://github.com/JonathanCMitchell/mobilenet_v2_keras/'\n",
        "                    'releases/download/v1.1/')\n",
        "\n",
        "backend = None\n",
        "layers = None\n",
        "models = None\n",
        "keras_utils = None\n",
        "\n",
        "\n",
        "_KERAS_BACKEND = None\n",
        "_KERAS_LAYERS = None\n",
        "_KERAS_MODELS = None\n",
        "_KERAS_UTILS = None\n",
        "\n",
        "\n",
        "def get_submodules_from_kwargs(kwargs):\n",
        "    backend = kwargs.get('backend', _KERAS_BACKEND)\n",
        "    layers = kwargs.get('layers', _KERAS_LAYERS)\n",
        "    models = kwargs.get('models', _KERAS_MODELS)\n",
        "    utils = kwargs.get('utils', _KERAS_UTILS)\n",
        "    for key in kwargs.keys():\n",
        "        if key not in ['backend', 'layers', 'models', 'utils']:\n",
        "            raise TypeError('Invalid keyword argument: %s', key)\n",
        "    return backend, layers, models, utils\n",
        "\n",
        "\n",
        "\n",
        "# This function is taken from the original tf repo.\n",
        "# It ensures that all layers have a channel number that is divisible by 8\n",
        "# It can be seen here:\n",
        "# https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "def keras_modules_injection(base_fun):\n",
        "\n",
        "    def wrapper(*args, **kwargs):\n",
        "        kwargs['backend'] = backend\n",
        "        kwargs['layers'] = layers\n",
        "        kwargs['models'] = models\n",
        "        kwargs['utils'] = utils\n",
        "        return base_fun(*args, **kwargs)\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "@keras_modules_injection\n",
        "def MobileNetV2(input_shape=None,\n",
        "                alpha=1.0,\n",
        "                include_top=True,\n",
        "                weights='imagenet',\n",
        "                input_tensor=None,\n",
        "                pooling=None,\n",
        "                classes=1000,\n",
        "                **kwargs):\n",
        "    \"\"\"Instantiates the MobileNetV2 architecture.\n",
        "    # Arguments\n",
        "        input_shape: optional shape tuple, to be specified if you would\n",
        "            like to use a model with an input img resolution that is not\n",
        "            (224, 224, 3).\n",
        "            It should have exactly 3 inputs channels (224, 224, 3).\n",
        "            You can also omit this option if you would like\n",
        "            to infer input_shape from an input_tensor.\n",
        "            If you choose to include both input_tensor and input_shape then\n",
        "            input_shape will be used if they match, if the shapes\n",
        "            do not match then we will throw an error.\n",
        "            E.g. `(160, 160, 3)` would be one valid value.\n",
        "        alpha: controls the width of the network. This is known as the\n",
        "        width multiplier in the MobileNetV2 paper, but the name is kept for\n",
        "        consistency with MobileNetV1 in Keras.\n",
        "            - If `alpha` < 1.0, proportionally decreases the number\n",
        "                of filters in each layer.\n",
        "            - If `alpha` > 1.0, proportionally increases the number\n",
        "                of filters in each layer.\n",
        "            - If `alpha` = 1, default number of filters from the paper\n",
        "                 are used at each layer.\n",
        "        include_top: whether to include the fully-connected\n",
        "            layer at the top of the network.\n",
        "        weights: one of `None` (random initialization),\n",
        "              'imagenet' (pre-training on ImageNet),\n",
        "              or the path to the weights file to be loaded.\n",
        "        input_tensor: optional Keras tensor (i.e. output of\n",
        "            `layers.Input()`)\n",
        "            to use as image input for the model.\n",
        "        pooling: Optional pooling mode for feature extraction\n",
        "            when `include_top` is `False`.\n",
        "            - `None` means that the output of the model\n",
        "                will be the 4D tensor output of the\n",
        "                last convolutional block.\n",
        "            - `avg` means that global average pooling\n",
        "                will be applied to the output of the\n",
        "                last convolutional block, and thus\n",
        "                the output of the model will be a\n",
        "                2D tensor.\n",
        "            - `max` means that global max pooling will\n",
        "                be applied.\n",
        "        classes: optional number of classes to classify images\n",
        "            into, only to be specified if `include_top` is True, and\n",
        "            if no `weights` argument is specified.\n",
        "    # Returns\n",
        "        A Keras model instance.\n",
        "    # Raises\n",
        "        ValueError: in case of invalid argument for `weights`,\n",
        "            or invalid input shape or invalid alpha, rows when\n",
        "            weights='imagenet'\n",
        "    \"\"\"\n",
        "    global backend, layers, models, keras_utils\n",
        "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
        "\n",
        "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization), `imagenet` '\n",
        "                         '(pre-training on ImageNet), '\n",
        "                         'or the path to the weights file to be loaded.')\n",
        "\n",
        "    if weights == 'imagenet' and include_top and classes != 1000:\n",
        "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top` '\n",
        "                         'as true, `classes` should be 1000')\n",
        "\n",
        "    # Determine proper input shape and default size.\n",
        "    # If both input_shape and input_tensor are used, they should match\n",
        "    if input_shape is not None and input_tensor is not None:\n",
        "        try:\n",
        "            is_input_t_tensor = backend.is_keras_tensor(input_tensor)\n",
        "        except ValueError:\n",
        "            try:\n",
        "                is_input_t_tensor = backend.is_keras_tensor(\n",
        "                    keras_utils.get_source_inputs(input_tensor))\n",
        "            except ValueError:\n",
        "                raise ValueError('input_tensor: ', input_tensor,\n",
        "                                 'is not type input_tensor')\n",
        "        if is_input_t_tensor:\n",
        "            if backend.image_data_format == 'channels_first':\n",
        "                if backend.int_shape(input_tensor)[1] != input_shape[1]:\n",
        "                    raise ValueError('input_shape: ', input_shape,\n",
        "                                     'and input_tensor: ', input_tensor,\n",
        "                                     'do not meet the same shape requirements')\n",
        "            else:\n",
        "                if backend.int_shape(input_tensor)[2] != input_shape[1]:\n",
        "                    raise ValueError('input_shape: ', input_shape,\n",
        "                                     'and input_tensor: ', input_tensor,\n",
        "                                     'do not meet the same shape requirements')\n",
        "        else:\n",
        "            raise ValueError('input_tensor specified: ', input_tensor,\n",
        "                             'is not a keras tensor')\n",
        "\n",
        "    # If input_shape is None, infer shape from input_tensor\n",
        "    if input_shape is None and input_tensor is not None:\n",
        "\n",
        "        try:\n",
        "            backend.is_keras_tensor(input_tensor)\n",
        "        except ValueError:\n",
        "            raise ValueError('input_tensor: ', input_tensor,\n",
        "                             'is type: ', type(input_tensor),\n",
        "                             'which is not a valid type')\n",
        "\n",
        "        if input_shape is None and not backend.is_keras_tensor(input_tensor):\n",
        "            default_size = 224\n",
        "        elif input_shape is None and backend.is_keras_tensor(input_tensor):\n",
        "            if backend.image_data_format() == 'channels_first':\n",
        "                rows = backend.int_shape(input_tensor)[2]\n",
        "                cols = backend.int_shape(input_tensor)[3]\n",
        "            else:\n",
        "                rows = backend.int_shape(input_tensor)[1]\n",
        "                cols = backend.int_shape(input_tensor)[2]\n",
        "\n",
        "            if rows == cols and rows in [96, 128, 160, 192, 224]:\n",
        "                default_size = rows\n",
        "            else:\n",
        "                default_size = 224\n",
        "\n",
        "    # If input_shape is None and no input_tensor\n",
        "    elif input_shape is None:\n",
        "        default_size = 224\n",
        "\n",
        "    # If input_shape is not None, assume default size\n",
        "    else:\n",
        "        if backend.image_data_format() == 'channels_first':\n",
        "            rows = input_shape[1]\n",
        "            cols = input_shape[2]\n",
        "        else:\n",
        "            rows = input_shape[0]\n",
        "            cols = input_shape[1]\n",
        "\n",
        "        if rows == cols and rows in [96, 128, 160, 192, 224]:\n",
        "            default_size = rows\n",
        "        else:\n",
        "            default_size = 224\n",
        "\n",
        "    input_shape = _obtain_input_shape(input_shape,\n",
        "                                      default_size=default_size,\n",
        "                                      min_size=32,\n",
        "                                      data_format=backend.image_data_format(),\n",
        "                                      require_flatten=include_top,\n",
        "                                      weights=weights)\n",
        "\n",
        "    if backend.image_data_format() == 'channels_last':\n",
        "        row_axis, col_axis = (0, 1)\n",
        "    else:\n",
        "        row_axis, col_axis = (1, 2)\n",
        "    rows = input_shape[row_axis]\n",
        "    cols = input_shape[col_axis]\n",
        "\n",
        "    if weights == 'imagenet':\n",
        "        if alpha not in [0.35, 0.50, 0.75, 1.0, 1.3, 1.4]:\n",
        "            raise ValueError('If imagenet weights are being loaded, '\n",
        "                             'alpha can be one of `0.35`, `0.50`, `0.75`, '\n",
        "                             '`1.0`, `1.3` or `1.4` only.')\n",
        "\n",
        "        if rows != cols or rows not in [96, 128, 160, 192, 224]:\n",
        "            rows = 224\n",
        "            warnings.warn('`input_shape` is undefined or non-square, '\n",
        "                          'or `rows` is not in [96, 128, 160, 192, 224].'\n",
        "                          ' Weights for input shape (224, 224) will be'\n",
        "                          ' loaded as the default.')\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = layers.Input(shape=input_shape)\n",
        "    else:\n",
        "        if not backend.is_keras_tensor(input_tensor):\n",
        "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    channel_axis = 1 if backend.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    first_block_filters = _make_divisible(32 * alpha, 8)\n",
        "    x = layers.ZeroPadding2D(padding=correct_pad(backend, img_input, 3),\n",
        "                             name='Conv1_pad')(img_input)\n",
        "    x = layers.Conv2D(first_block_filters,\n",
        "                      kernel_size=3,\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      use_bias=False,\n",
        "                      name='Conv1')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                  epsilon=1e-3,\n",
        "                                  momentum=0.999,\n",
        "                                  name='bn_Conv1')(x)\n",
        "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n",
        "                            expansion=1, block_id=0)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=1)\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=2)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=3)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=4)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=5)\n",
        "\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,\n",
        "    #                         expansion=6, block_id=6)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=7)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=8)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=9)\n",
        "\n",
        "    # x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=10)\n",
        "    # x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=11)\n",
        "    # x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=12)\n",
        "\n",
        "    # x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2,\n",
        "    #                         expansion=6, block_id=13)\n",
        "    # x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=14)\n",
        "    # x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=15)\n",
        "\n",
        "    # x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1,\n",
        "    #                         expansion=6, block_id=16)\n",
        "\n",
        "    # no alpha applied to last conv as stated in the paper:\n",
        "    # if the width multiplier is greater than 1 we\n",
        "    # increase the number of output channels\n",
        "    if alpha > 1.0:\n",
        "        last_block_filters = _make_divisible(1280 * alpha, 8)\n",
        "    else:\n",
        "        last_block_filters = 1280\n",
        "\n",
        "    x = layers.Conv2D(last_block_filters,\n",
        "                      kernel_size=1,\n",
        "                      use_bias=False,\n",
        "                      name='Conv_1')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                  epsilon=1e-3,\n",
        "                                  momentum=0.999,\n",
        "                                  name='Conv_1_bn')(x)\n",
        "    x = layers.ReLU(6., name='out_relu')(x)\n",
        "\n",
        "    if include_top:\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "        x = layers.Dense(classes, activation='softmax',\n",
        "                         use_bias=True, name='Logits')(x)\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = layers.GlobalAveragePooling2D()(x)\n",
        "        elif pooling == 'max':\n",
        "            x = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = models.Model(inputs, x,\n",
        "                         name='mobilenetv2_%0.2f_%s' % (alpha, rows))\n",
        "\n",
        "    # Load weights.\n",
        "    if weights == 'imagenet':\n",
        "        if include_top:\n",
        "            model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' +\n",
        "                          str(alpha) + '_' + str(rows) + '.h5')\n",
        "            weight_path = BASE_WEIGHT_PATH + model_name\n",
        "            weights_path = keras_utils.get_file(\n",
        "                model_name, weight_path, cache_subdir='models')\n",
        "        else:\n",
        "            model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' +\n",
        "                          str(alpha) + '_' + str(rows) + '_no_top' + '.h5')\n",
        "            weight_path = BASE_WEIGHT_PATH + model_name\n",
        "            weights_path = keras_utils.get_file(\n",
        "                model_name, weight_path, cache_subdir='models')\n",
        "        model.load_weights(weights_path)\n",
        "    elif weights is not None:\n",
        "        model.load_weights(weights)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
        "    channel_axis = 1 if backend.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    in_channels = backend.int_shape(inputs)[channel_axis]\n",
        "    pointwise_conv_filters = int(filters * alpha)\n",
        "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
        "    x = inputs\n",
        "    prefix = 'block_{}_'.format(block_id)\n",
        "\n",
        "    if block_id:\n",
        "        # Expand\n",
        "        x = layers.Conv2D(expansion * in_channels,\n",
        "                          kernel_size=1,\n",
        "                          padding='same',\n",
        "                          use_bias=False,\n",
        "                          activation=None,\n",
        "                          name=prefix + 'expand')(x)\n",
        "        x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                      epsilon=1e-3,\n",
        "                                      momentum=0.999,\n",
        "                                      name=prefix + 'expand_BN')(x)\n",
        "        x = layers.ReLU(6., name=prefix + 'expand_relu')(x)\n",
        "    else:\n",
        "        prefix = 'expanded_conv_'\n",
        "\n",
        "    # Depthwise\n",
        "    if stride == 2:\n",
        "        x = layers.ZeroPadding2D(padding=correct_pad(backend, x, 3),\n",
        "                                 name=prefix + 'pad')(x)\n",
        "    x = layers.DepthwiseConv2D(kernel_size=3,\n",
        "                               strides=stride,\n",
        "                               activation=None,\n",
        "                               use_bias=False,\n",
        "                               padding='same' if stride == 1 else 'valid',\n",
        "                               name=prefix + 'depthwise')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                  epsilon=1e-3,\n",
        "                                  momentum=0.999,\n",
        "                                  name=prefix + 'depthwise_BN')(x)\n",
        "\n",
        "    x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
        "\n",
        "    # Project\n",
        "    x = layers.Conv2D(pointwise_filters,\n",
        "                      kernel_size=1,\n",
        "                      padding='same',\n",
        "                      use_bias=False,\n",
        "                      activation=None,\n",
        "                      name=prefix + 'project')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis,\n",
        "                                  epsilon=1e-3,\n",
        "                                  momentum=0.999,\n",
        "                                  name=prefix + 'project_BN')(x)\n",
        "\n",
        "    if in_channels == pointwise_filters and stride == 1:\n",
        "        return layers.Add(name=prefix + 'add')([inputs, x])\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def _obtain_input_shape(input_shape,\n",
        "                        default_size,\n",
        "                        min_size,\n",
        "                        data_format,\n",
        "                        require_flatten,\n",
        "                        weights=None):\n",
        "    \"\"\"Internal utility to compute/validate a model's input shape.\n",
        "    # Arguments\n",
        "        input_shape: Either None (will return the default network input shape),\n",
        "            or a user-provided shape to be validated.\n",
        "        default_size: Default input width/height for the model.\n",
        "        min_size: Minimum input width/height accepted by the model.\n",
        "        data_format: Image data format to use.\n",
        "        require_flatten: Whether the model is expected to\n",
        "            be linked to a classifier via a Flatten layer.\n",
        "        weights: One of `None` (random initialization)\n",
        "            or 'imagenet' (pre-training on ImageNet).\n",
        "            If weights='imagenet' input channels must be equal to 3.\n",
        "    # Returns\n",
        "        An integer shape tuple (may include None entries).\n",
        "    # Raises\n",
        "        ValueError: In case of invalid argument values.\n",
        "    \"\"\"\n",
        "    if weights != 'imagenet' and input_shape and len(input_shape) == 3:\n",
        "        if data_format == 'channels_first':\n",
        "            if input_shape[0] not in {1, 3}:\n",
        "                warnings.warn(\n",
        "                    'This model usually expects 1 or 3 input channels. '\n",
        "                    'However, it was passed an input_shape with ' +\n",
        "                    str(input_shape[0]) + ' input channels.')\n",
        "            default_shape = (input_shape[0], default_size, default_size)\n",
        "        else:\n",
        "            if input_shape[-1] not in {1, 3}:\n",
        "                warnings.warn(\n",
        "                    'This model usually expects 1 or 3 input channels. '\n",
        "                    'However, it was passed an input_shape with ' +\n",
        "                    str(input_shape[-1]) + ' input channels.')\n",
        "            default_shape = (default_size, default_size, input_shape[-1])\n",
        "    else:\n",
        "        if data_format == 'channels_first':\n",
        "            default_shape = (3, default_size, default_size)\n",
        "        else:\n",
        "            default_shape = (default_size, default_size, 3)\n",
        "    if weights == 'imagenet' and require_flatten:\n",
        "        if input_shape is not None:\n",
        "            if input_shape != default_shape:\n",
        "                raise ValueError('When setting `include_top=True` '\n",
        "                                 'and loading `imagenet` weights, '\n",
        "                                 '`input_shape` should be ' +\n",
        "                                 str(default_shape) + '.')\n",
        "        return default_shape\n",
        "    if input_shape:\n",
        "        if data_format == 'channels_first':\n",
        "            if input_shape is not None:\n",
        "                if len(input_shape) != 3:\n",
        "                    raise ValueError(\n",
        "                        '`input_shape` must be a tuple of three integers.')\n",
        "                if input_shape[0] != 3 and weights == 'imagenet':\n",
        "                    raise ValueError('The input must have 3 channels; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "                if ((input_shape[1] is not None and input_shape[1] < min_size) or\n",
        "                   (input_shape[2] is not None and input_shape[2] < min_size)):\n",
        "                    raise ValueError('Input size must be at least ' +\n",
        "                                     str(min_size) + 'x' + str(min_size) +\n",
        "                                     '; got `input_shape=' +\n",
        "                                     str(input_shape) + '`')\n",
        "        else:\n",
        "            if input_shape is not None:\n",
        "                if len(input_shape) != 3:\n",
        "                    raise ValueError(\n",
        "                        '`input_shape` must be a tuple of three integers.')\n",
        "                if input_shape[-1] != 3 and weights == 'imagenet':\n",
        "                    raise ValueError('The input must have 3 channels; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "                if ((input_shape[0] is not None and input_shape[0] < min_size) or\n",
        "                   (input_shape[1] is not None and input_shape[1] < min_size)):\n",
        "                    raise ValueError('Input size must be at least ' +\n",
        "                                     str(min_size) + 'x' + str(min_size) +\n",
        "                                     '; got `input_shape=' +\n",
        "                                     str(input_shape) + '`')\n",
        "    else:\n",
        "        if require_flatten:\n",
        "            input_shape = default_shape\n",
        "        else:\n",
        "            if data_format == 'channels_first':\n",
        "                input_shape = (3, None, None)\n",
        "            else:\n",
        "                input_shape = (None, None, 3)\n",
        "    if require_flatten:\n",
        "        if None in input_shape:\n",
        "            raise ValueError('If `include_top` is True, '\n",
        "                             'you should specify a static `input_shape`. '\n",
        "                             'Got `input_shape=' + str(input_shape) + '`')\n",
        "    return input_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRdHApE0Dmrp",
        "colab_type": "text"
      },
      "source": [
        "### Generate model. We need to compile this model and eventually train for one loop to workaround what may be a possible bug around weights initialization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDW3Wq0GKYBh",
        "colab_type": "code",
        "outputId": "07e382b7-e169-497b-c490-55a1ec0da7c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from cleverhans.utils_keras import KerasModelWrapper\n",
        "\n",
        "def get_mnv2(alpha=1):\n",
        "  # All parameters are default except weights = None because we will be training this ourselves on cifar10, not ImageNet, classes=10. From the docs:\n",
        "  # https://keras.io/applications/#mobilenetv2\n",
        "  \n",
        "  # x = tensorflow.keras.layers.Input(shape=(96,96,3))\n",
        "  model = MobileNetV2(input_shape=(32,32,3), alpha=alpha, include_top=True, weights=None, input_tensor=None, pooling=None, classes=10)\n",
        "\n",
        "  opt = tensorflow.keras.optimizers.RMSprop(learning_rate=0.0001, rho=1)\n",
        "\n",
        "  model.compile(optimizer='RMSProp',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  # for layer in model.layers:\n",
        "  # print(layer.trainable)\n",
        "\n",
        "  # x = tensorflow.keras.layers.Input(shape=(32,32,3))\n",
        "  # y = tensorflow.keras.layers.Flatten()(x)\n",
        "  # y = tensorflow.keras.layers.Dense(10, activation='softmax')(y)\n",
        "  \n",
        "  # model = tensorflow.keras.models.Model(inputs=x, outputs=y)\n",
        "  return model\n",
        "\n",
        "model = get_mnv2()\n",
        "print(model.summary())\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"mobilenetv2_1.00_32\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Conv1_pad (ZeroPadding2D)       (None, 33, 33, 3)    0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Conv1 (Conv2D)                  (None, 16, 16, 32)   864         Conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bn_Conv1 (BatchNormalization)   (None, 16, 16, 32)   128         Conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Conv1_relu (ReLU)               (None, 16, 16, 32)   0           bn_Conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise (Depthw (None, 16, 16, 32)   288         Conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise_BN (Bat (None, 16, 16, 32)   128         expanded_conv_depthwise[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise_relu (R (None, 16, 16, 32)   0           expanded_conv_depthwise_BN[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_project (Conv2D)  (None, 16, 16, 16)   512         expanded_conv_depthwise_relu[0][0\n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_project_BN (Batch (None, 16, 16, 16)   64          expanded_conv_project[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand (Conv2D)         (None, 16, 16, 96)   1536        expanded_conv_project_BN[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand_BN (BatchNormali (None, 16, 16, 96)   384         block_1_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand_relu (ReLU)      (None, 16, 16, 96)   0           block_1_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_pad (ZeroPadding2D)     (None, 17, 17, 96)   0           block_1_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise (DepthwiseCon (None, 8, 8, 96)     864         block_1_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise_BN (BatchNorm (None, 8, 8, 96)     384         block_1_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise_relu (ReLU)   (None, 8, 8, 96)     0           block_1_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_1_project (Conv2D)        (None, 8, 8, 24)     2304        block_1_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_1_project_BN (BatchNormal (None, 8, 8, 24)     96          block_1_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand (Conv2D)         (None, 8, 8, 144)    3456        block_1_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand_BN (BatchNormali (None, 8, 8, 144)    576         block_2_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand_relu (ReLU)      (None, 8, 8, 144)    0           block_2_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise (DepthwiseCon (None, 8, 8, 144)    1296        block_2_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise_BN (BatchNorm (None, 8, 8, 144)    576         block_2_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise_relu (ReLU)   (None, 8, 8, 144)    0           block_2_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_2_project (Conv2D)        (None, 8, 8, 24)     3456        block_2_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_2_project_BN (BatchNormal (None, 8, 8, 24)     96          block_2_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_2_add (Add)               (None, 8, 8, 24)     0           block_1_project_BN[0][0]         \n",
            "                                                                 block_2_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand (Conv2D)         (None, 8, 8, 144)    3456        block_2_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand_BN (BatchNormali (None, 8, 8, 144)    576         block_3_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand_relu (ReLU)      (None, 8, 8, 144)    0           block_3_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_pad (ZeroPadding2D)     (None, 9, 9, 144)    0           block_3_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise (DepthwiseCon (None, 4, 4, 144)    1296        block_3_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise_BN (BatchNorm (None, 4, 4, 144)    576         block_3_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise_relu (ReLU)   (None, 4, 4, 144)    0           block_3_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_3_project (Conv2D)        (None, 4, 4, 32)     4608        block_3_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_3_project_BN (BatchNormal (None, 4, 4, 32)     128         block_3_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand (Conv2D)         (None, 4, 4, 192)    6144        block_3_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand_BN (BatchNormali (None, 4, 4, 192)    768         block_4_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand_relu (ReLU)      (None, 4, 4, 192)    0           block_4_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise (DepthwiseCon (None, 4, 4, 192)    1728        block_4_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise_BN (BatchNorm (None, 4, 4, 192)    768         block_4_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise_relu (ReLU)   (None, 4, 4, 192)    0           block_4_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_4_project (Conv2D)        (None, 4, 4, 32)     6144        block_4_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_4_project_BN (BatchNormal (None, 4, 4, 32)     128         block_4_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_4_add (Add)               (None, 4, 4, 32)     0           block_3_project_BN[0][0]         \n",
            "                                                                 block_4_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand (Conv2D)         (None, 4, 4, 192)    6144        block_4_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand_BN (BatchNormali (None, 4, 4, 192)    768         block_5_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand_relu (ReLU)      (None, 4, 4, 192)    0           block_5_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise (DepthwiseCon (None, 4, 4, 192)    1728        block_5_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise_BN (BatchNorm (None, 4, 4, 192)    768         block_5_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise_relu (ReLU)   (None, 4, 4, 192)    0           block_5_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_5_project (Conv2D)        (None, 4, 4, 32)     6144        block_5_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_5_project_BN (BatchNormal (None, 4, 4, 32)     128         block_5_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_5_add (Add)               (None, 4, 4, 32)     0           block_4_add[0][0]                \n",
            "                                                                 block_5_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Conv_1 (Conv2D)                 (None, 4, 4, 1280)   40960       block_5_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Conv_1_bn (BatchNormalization)  (None, 4, 4, 1280)   5120        Conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "out_relu (ReLU)                 (None, 4, 4, 1280)   0           Conv_1_bn[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_5 (Glo (None, 1280)         0           out_relu[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Logits (Dense)                  (None, 10)           12810       global_average_pooling2d_5[0][0] \n",
            "==================================================================================================\n",
            "Total params: 117,898\n",
            "Trainable params: 111,818\n",
            "Non-trainable params: 6,080\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bevLmhN3yH-",
        "colab_type": "text"
      },
      "source": [
        "### Train mobilenetv2, run pgd on it. \n",
        "\n",
        "This is mostly from the Cleverhans tutorial on Keras models. \n",
        "\n",
        "There is one unexpected trick here. The model needs to be trained in one loop of Keras training before handing it over to the Cleverhans training loop. It may be that an initial run may be needed to get the TF variables listed out properly (e.g. dry run).  \n",
        "\n",
        "The truncated MobileNetV2 model built With an alpha of 1.0 generates these stats:\n",
        "\n",
        "<pre>\n",
        "Total params: 117,898\n",
        "Trainable params: 111,818\n",
        "Non-trainable params: 6,080\n",
        "</pre>\n",
        "\n",
        "71% top-1 accuracy on CIFAR-10 after 20 epochs. \n",
        "\n",
        "<pre>\n",
        "[INFO 2020-01-22 19:13:36,698 cleverhans] Epoch 20 took 11.61600112915039 seconds\n",
        "Test accuracy on legitimate examples: 0.715000\n",
        "[INFO 2020-01-22 19:13:49,190 cleverhans] Epoch 21 took 11.589605331420898 seconds\n",
        "Test accuracy on legitimate examples: 0.717500\n",
        "[INFO 2020-01-22 19:14:01,381 cleverhans] Epoch 22 took 11.313880681991577 seconds\n",
        "Test accuracy on legitimate examples: 0.711500\n",
        "[INFO 2020-01-22 19:14:13,840 cleverhans] Epoch 23 took 11.53160548210144 seconds\n",
        "Test accuracy on legitimate examples: 0.706500\n",
        "[INFO 2020-01-22 19:14:26,153 cleverhans] Epoch 24 took 11.455431938171387 seconds\n",
        "Test accuracy on legitimate examples: 0.710800\n",
        "</pre>\n",
        "\n",
        "\n",
        "Adversarial retraining with Madry epsilon 4 (= 4/255) increases both clean and adversarial accuracy to roughly 64%/38% by epoch 15. \n",
        "\n",
        "<pre>\n",
        "[INFO 2020-01-22 19:48:54,746 cleverhans] Epoch 14 took 109.258061170578 seconds\n",
        "Test accuracy on legitimate examples: 0.6446\n",
        "Test accuracy on adversarial examples: 0.3801\n",
        "[INFO 2020-01-22 19:51:03,782 cleverhans] Epoch 15 took 110.1007444858551 seconds\n",
        "Test accuracy on legitimate examples: 0.6157\n",
        "Test accuracy on adversarial examples: 0.3743\n",
        "[INFO 2020-01-22 19:53:12,622 cleverhans] Epoch 16 took 110.15545964241028 seconds\n",
        "Test accuracy on legitimate examples: 0.6336\n",
        "Test accuracy on adversarial examples: 0.3764\n",
        "[INFO 2020-01-22 19:55:20,412 cleverhans] Epoch 17 took 108.51104092597961 seconds\n",
        "Test accuracy on legitimate examples: 0.6397\n",
        "Test accuracy on adversarial examples: 0.3824\n",
        "</pre>\n",
        "-------------\n",
        "\n",
        "\n",
        "The truncated MobileNetV2 model built with an alpha of 0.35 generates these stats:\n",
        "\n",
        "<pre>\n",
        "Total params: 54,074\n",
        "Trainable params: 49,962\n",
        "Non-trainable params: 4,112\n",
        "</pre>\n",
        "\n",
        "65% top-1 accuracy on CIFAR-10 after about 15 epochs. \n",
        "\n",
        "<pre>\n",
        "[INFO 2020-01-21 22:06:11,473 cleverhans] Epoch 14 took 11.071139812469482 seconds\n",
        "Test accuracy on legitimate examples: 0.644500\n",
        "[INFO 2020-01-21 22:06:23,392 cleverhans] Epoch 15 took 11.085895776748657 seconds\n",
        "Test accuracy on legitimate examples: 0.646700\n",
        "[INFO 2020-01-21 22:06:35,024 cleverhans] Epoch 16 took 10.808518648147583 seconds\n",
        "Test accuracy on legitimate examples: 0.653600\n",
        "[INFO 2020-01-21 22:06:47,575 cleverhans] Epoch 17 took 11.663419008255005 seconds\n",
        "Test accuracy on legitimate examples: 0.646000\n",
        "[INFO 2020-01-21 22:06:59,908 cleverhans] Epoch 18 took 11.490140199661255 seconds\n",
        "Test accuracy on legitimate examples: 0.643300\n",
        "[INFO 2020-01-21 22:07:11,687 cleverhans] Epoch 19 took 10.94323444366455 seconds\n",
        "Test accuracy on legitimate examples: 0.658200\n",
        "</pre>\n",
        "\n",
        "Adversarial retraining with Madry epsilon 4 (= 4/255) increases both clean and adversarial accuracy.\n",
        "\n",
        "<pre>\n",
        "[INFO 2020-01-22 01:08:49,993 cleverhans] Epoch 0 took 112.98240733146667 seconds\n",
        "Test accuracy on legitimate examples: 0.4187\n",
        "Test accuracy on adversarial examples: 0.2815\n",
        "[INFO 2020-01-22 01:11:07,334 cleverhans] Epoch 1 took 110.0533185005188 seconds\n",
        "Test accuracy on legitimate examples: 0.4538\n",
        "Test accuracy on adversarial examples: 0.3147\n",
        "...\n",
        "[INFO 2020-01-22 01:47:44,696 cleverhans] Epoch 18 took 109.45361185073853 seconds\n",
        "Test accuracy on legitimate examples: 0.5932\n",
        "Test accuracy on adversarial examples: 0.3619\n",
        "[INFO 2020-01-22 01:49:53,164 cleverhans] Epoch 19 took 109.46263790130615 seconds\n",
        "Test accuracy on legitimate examples: 0.6000\n",
        "Test accuracy on adversarial examples: 0.3624\n",
        "<cleverhans.utils.AccuracyReport at 0x7f5ec5400eb8>\n",
        "</pre>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pszj9OnX20C2",
        "colab_type": "code",
        "outputId": "32b89409-d5f1-4656-bbda-63764a6372c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# from cleverhans.attacks import FastGradientMethod\n",
        "from cleverhans.attacks import MadryEtAl\n",
        "from cleverhans.compat import flags\n",
        "# from cleverhans.dataset import MNIST\n",
        "from cleverhans.dataset import CIFAR10\n",
        "from cleverhans.loss import CrossEntropy\n",
        "from cleverhans.train import train\n",
        "from cleverhans.utils import AccuracyReport\n",
        "from cleverhans.utils_keras import cnn_model\n",
        "from cleverhans.utils_keras import KerasModelWrapper\n",
        "from cleverhans.utils_tf import model_eval\n",
        "\n",
        "def cifar_pgd(     train_start=   0, \n",
        "                   train_end=     50000, \n",
        "                   test_start=    0,\n",
        "                   test_end=      10000, \n",
        "                   nb_epochs=     20,\n",
        "                   batch_size=    64,\n",
        "                   learning_rate= 0.001, \n",
        "                   train_dir=     \".\",\n",
        "                   filename=      \"./cleverhansout\",\n",
        "                   load_model=    False,\n",
        "                   testing=       True, \n",
        "                   label_smoothing=0.0):\n",
        "  \"\"\"\n",
        "  Derived from MNIST CleverHans tutorial\n",
        "\n",
        "  Adjusted to use Keras MobileNet V2 model, CIFAR, and PGD.\n",
        "\n",
        "  :param train_start: index of first training set example\n",
        "  :param train_end: index of last training set example\n",
        "  :param test_start: index of first test set example\n",
        "  :param test_end: index of last test set example\n",
        "  :param nb_epochs: number of epochs to train model\n",
        "  :param batch_size: size of training batches\n",
        "  :param learning_rate: learning rate for training\n",
        "  :param train_dir: Directory storing the saved model\n",
        "  :param filename: Filename to save model under\n",
        "  :param load_model: True for load, False for not load\n",
        "  :param testing: if true, test error is calculated\n",
        "  :param label_smoothing: float, amount of label smoothing for cross entropy\n",
        "  :return: an AccuracyReport object\n",
        "  \"\"\"\n",
        "  tf.keras.backend.set_learning_phase(0)\n",
        "\n",
        "  # Object used to keep track of (and return) key accuracies\n",
        "  report = AccuracyReport()\n",
        "\n",
        "  # Set TF random seed to improve reproducibility\n",
        "  tf.set_random_seed(1234)\n",
        "\n",
        "  if keras.backend.image_data_format() != 'channels_last':\n",
        "    raise NotImplementedError(\"this tutorial requires keras to be configured to channels_last format\")\n",
        "\n",
        "  # Create TF session and set as Keras backend session\n",
        "  with tf.Session() as sess:\n",
        "\n",
        "  # sess = tf.Session()\n",
        "    keras.backend.set_session(sess)\n",
        "\n",
        "    # Get test data\n",
        "    cifar = CIFAR10(train_start=train_start, train_end=train_end,\n",
        "                  test_start=test_start, test_end=test_end)\n",
        "    x_train, y_train = cifar.get_set('train')\n",
        "    x_test, y_test = cifar.get_set('test')\n",
        "\n",
        "    # Obtain Image Parameters\n",
        "    img_rows, img_cols, nchannels = x_train.shape[1:4]\n",
        "    nb_classes = y_train.shape[1]\n",
        "\n",
        "    # Define input TF placeholder\n",
        "    x = tf.placeholder(tf.float32, shape=(None, img_rows, img_cols,\n",
        "                                          nchannels))\n",
        "    y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
        "\n",
        "    # Define TF model graph\n",
        "    # model = cnn_model(img_rows=img_rows, img_cols=img_cols,\n",
        "    #                   channels=nchannels, nb_filters=64,\n",
        "    #                   nb_classes=nb_classes)\n",
        "    model = get_mnv2()\n",
        "    preds = model(x)\n",
        "    print(\"Defined TensorFlow model graph.\")\n",
        "\n",
        "    def evaluate():\n",
        "      # Evaluate the accuracy of the model on legitimate test examples\n",
        "      eval_params = {'batch_size': batch_size}\n",
        "      acc = model_eval(sess, x, y, preds, x_test, y_test, args=eval_params)\n",
        "      report.clean_train_clean_eval = acc\n",
        "    #        assert X_test.shape[0] == test_end - test_start, X_test.shape\n",
        "      print('Test accuracy on legitimate examples: %0.6f' % acc)\n",
        "\n",
        "    # Train model\n",
        "    train_params = {\n",
        "        'nb_epochs': nb_epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'learning_rate': learning_rate,\n",
        "        'train_dir': train_dir,\n",
        "        'filename': filename\n",
        "    }\n",
        "\n",
        "    rng = np.random.RandomState([2017, 8, 30])\n",
        "    if not os.path.exists(train_dir):\n",
        "      os.mkdir(train_dir)\n",
        "\n",
        "    ckpt = tf.train.get_checkpoint_state(train_dir)\n",
        "    print(train_dir, ckpt)\n",
        "    ckpt_path = False if ckpt is None else ckpt.model_checkpoint_path\n",
        "    wrap = KerasModelWrapper(model)\n",
        "\n",
        "    if load_model and ckpt_path:\n",
        "      saver = tf.train.Saver()\n",
        "      print(ckpt_path)\n",
        "      saver.restore(sess, ckpt_path)\n",
        "      print(\"Model loaded from: {}\".format(ckpt_path))\n",
        "      evaluate()\n",
        "    else:\n",
        "      print(\"Model was not loaded, training from scratch.\")\n",
        "      loss = CrossEntropy(wrap, smoothing=label_smoothing)\n",
        "\n",
        "      # Do one cycle of Keras training. This may be to work around a \n",
        "      # possible bug in weight non-initialization.\n",
        "      #\n",
        "      model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=1, verbose=1, \n",
        "                callbacks=None, validation_split=0.0, validation_data=None, \n",
        "                shuffle=True, class_weight=None, sample_weight=None, \n",
        "                initial_epoch=0, steps_per_epoch=None, validation_steps=None, \n",
        "                validation_freq=1, max_queue_size=10, workers=1, \n",
        "                use_multiprocessing=False)\n",
        "\n",
        "      # rms = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "      train(sess, loss, x_train, y_train, evaluate=evaluate,\n",
        "            # optimizer=rms,\n",
        "            args=train_params, rng=rng)\n",
        "\n",
        "    # Calculate training error\n",
        "    if testing:\n",
        "      eval_params = {'batch_size': batch_size}\n",
        "      acc = model_eval(sess, x, y, preds, x_train, y_train, args=eval_params)\n",
        "      report.train_clean_train_clean_eval = acc\n",
        "\n",
        "    # Initialize the attack object and graph\n",
        "    madry = MadryEtAl(wrap, sess=sess)\n",
        "    # TODO: Check to see that these params are application to Madry. These were lifted from FGSM.\n",
        "    madry_params = {'eps': 8./255., # 0.3,\n",
        "                    'eps_iter': 2./255.,\n",
        "                    'clip_min': 0.,\n",
        "                    'clip_max': 1.}\n",
        "    adv_x = madry.generate(x, **madry_params)\n",
        "    # Consider the attack to be constant\n",
        "    adv_x = tf.stop_gradient(adv_x)\n",
        "    preds_adv = model(adv_x)\n",
        "\n",
        "    # Evaluate the accuracy of the MNIST model on adversarial examples\n",
        "    eval_par = {'batch_size': batch_size}\n",
        "    acc = model_eval(sess, x, y, preds_adv, x_test, y_test, args=eval_par)\n",
        "    print('Test accuracy on adversarial examples: %0.6f\\n' % acc)\n",
        "    report.clean_train_adv_eval = acc\n",
        "\n",
        "    # Calculating train error\n",
        "    if testing:\n",
        "      eval_par = {'batch_size': batch_size}\n",
        "      acc = model_eval(sess, x, y, preds_adv, x_train,\n",
        "                        y_train, args=eval_par)\n",
        "      report.train_clean_train_adv_eval = acc\n",
        "\n",
        "    print(\"Repeating the process, using adversarial training\")\n",
        "    # Redefine TF model graph\n",
        "    # Ensue that the model is new. \n",
        "    model_2 = get_mnv2()\n",
        "    wrap_2 = KerasModelWrapper(model_2)\n",
        "    preds_2 = model_2(x)\n",
        "    madry2 = MadryEtAl(wrap_2, sess=sess)\n",
        "\n",
        "    model_2.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=1, verbose=1, \n",
        "              callbacks=None, validation_split=0.0, validation_data=None, \n",
        "              shuffle=True, class_weight=None, sample_weight=None, \n",
        "              initial_epoch=0, steps_per_epoch=None, validation_steps=None, \n",
        "              validation_freq=1, max_queue_size=10, workers=1, \n",
        "              use_multiprocessing=False)\n",
        "\n",
        "    def attack(x):\n",
        "      return madry2.generate(x, **madry_params)\n",
        "\n",
        "    preds_2_adv = model_2(attack(x))\n",
        "    loss_2 = CrossEntropy(wrap_2, smoothing=label_smoothing, attack=attack)\n",
        "\n",
        "    def evaluate_2():\n",
        "      # Accuracy of adversarially trained model on legitimate test inputs\n",
        "      eval_params = {'batch_size': batch_size}\n",
        "      accuracy = model_eval(sess, x, y, preds_2, x_test, y_test,\n",
        "                            args=eval_params)\n",
        "      print('Test accuracy on legitimate examples: %0.4f' % accuracy)\n",
        "      report.adv_train_clean_eval = accuracy\n",
        "\n",
        "      # Accuracy of the adversarially trained model on adversarial examples\n",
        "      accuracy = model_eval(sess, x, y, preds_2_adv, x_test,\n",
        "                            y_test, args=eval_params)\n",
        "      print('Test accuracy on adversarial examples: %0.4f' % accuracy)\n",
        "      report.adv_train_adv_eval = accuracy\n",
        "\n",
        "    # Perform and evaluate adversarial training\n",
        "    train(sess, loss_2, x_train, y_train, evaluate=evaluate_2,\n",
        "          args=train_params, rng=rng)\n",
        "\n",
        "    # Calculate training errors\n",
        "    if testing:\n",
        "      eval_params = {'batch_size': batch_size}\n",
        "      accuracy = model_eval(sess, x, y, preds_2, x_train, y_train,\n",
        "                            args=eval_params)\n",
        "      report.train_adv_train_clean_eval = accuracy\n",
        "      accuracy = model_eval(sess, x, y, preds_2_adv, x_train,\n",
        "                            y_train, args=eval_params)\n",
        "      report.train_adv_train_adv_eval = accuracy\n",
        "    # end with tf.Session():\n",
        "\n",
        "  return report\n",
        "\n",
        "cifar_pgd(batch_size=32, nb_epochs=25)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "Defined TensorFlow model graph.\n",
            ". None\n",
            "Model was not loaded, training from scratch.\n",
            "Train on 50000 samples\n",
            "50000/50000 [==============================] - 20s 390us/sample - loss: 1.9919 - acc: 0.2487\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:09:25,505 cleverhans] Epoch 0 took 12.633360624313354 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.436000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:09:40,356 cleverhans] Epoch 1 took 11.573631048202515 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.507100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:09:52,665 cleverhans] Epoch 2 took 11.377753257751465 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.579400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:10:05,083 cleverhans] Epoch 3 took 11.475052833557129 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.594000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:10:17,712 cleverhans] Epoch 4 took 11.789032936096191 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.624000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:10:30,246 cleverhans] Epoch 5 took 11.647308826446533 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.648200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:10:42,958 cleverhans] Epoch 6 took 11.75253415107727 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.650800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:10:55,257 cleverhans] Epoch 7 took 11.44093132019043 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.671200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:11:07,616 cleverhans] Epoch 8 took 11.495986700057983 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.671700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:11:20,120 cleverhans] Epoch 9 took 11.686120986938477 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.681700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:11:32,396 cleverhans] Epoch 10 took 11.447280883789062 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.692600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:11:44,970 cleverhans] Epoch 11 took 11.643674373626709 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.684200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:11:57,611 cleverhans] Epoch 12 took 11.774304151535034 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.681500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:12:10,033 cleverhans] Epoch 13 took 11.53203296661377 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.702400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:12:22,494 cleverhans] Epoch 14 took 11.617349624633789 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.704800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:12:34,761 cleverhans] Epoch 15 took 11.387998342514038 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.708700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:12:47,191 cleverhans] Epoch 16 took 11.581614971160889 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.694800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:12:59,519 cleverhans] Epoch 17 took 11.44175910949707 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.690600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:13:11,783 cleverhans] Epoch 18 took 11.386084079742432 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.703200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:13:24,231 cleverhans] Epoch 19 took 11.571010828018188 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.707800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:13:36,698 cleverhans] Epoch 20 took 11.61600112915039 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.715000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:13:49,190 cleverhans] Epoch 21 took 11.589605331420898 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.717500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:14:01,381 cleverhans] Epoch 22 took 11.313880681991577 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.711500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:14:13,840 cleverhans] Epoch 23 took 11.53160548210144 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.706500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:14:26,153 cleverhans] Epoch 24 took 11.455431938171387 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.710800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/src/cleverhans/cleverhans/compat.py:22: UserWarning: <function reduce_sum_v1 at 0x7fd5d0cf5598> is deprecated. Switch to calling the equivalent function in tensorflow.  This function was originally needed as a compatibility layer for old versions of tensorflow,  but support for those versions has now been dropped.\n",
            "  warnings.warn(str(f) + \" is deprecated. Switch to calling the equivalent function in tensorflow. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on adversarial examples: 0.133400\n",
            "\n",
            "Repeating the process, using adversarial training\n",
            "Train on 50000 samples\n",
            "50000/50000 [==============================] - 20s 406us/sample - loss: 2.0196 - acc: 0.2323\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/src/cleverhans/cleverhans/compat.py:22: UserWarning: <function reduce_sum_v1 at 0x7fd5d0cf5598> is deprecated. Switch to calling the equivalent function in tensorflow.  This function was originally needed as a compatibility layer for old versions of tensorflow,  but support for those versions has now been dropped.\n",
            "  warnings.warn(str(f) + \" is deprecated. Switch to calling the equivalent function in tensorflow. \"\n",
            "/content/src/cleverhans/cleverhans/loss.py:41: UserWarning: callable attacks are deprecated, switch to an Attack subclass. callable attacks will not be supported after 2019-05-05.\n",
            "  warnings.warn(\"callable attacks are deprecated, switch to an Attack \"\n",
            "/content/src/cleverhans/cleverhans/compat.py:22: UserWarning: <function reduce_sum_v1 at 0x7fd5d0cf5598> is deprecated. Switch to calling the equivalent function in tensorflow.  This function was originally needed as a compatibility layer for old versions of tensorflow,  but support for those versions has now been dropped.\n",
            "  warnings.warn(str(f) + \" is deprecated. Switch to calling the equivalent function in tensorflow. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:18:54,341 cleverhans] Epoch 0 took 110.81437993049622 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.4475\n",
            "Test accuracy on adversarial examples: 0.2955\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:21:07,851 cleverhans] Epoch 1 took 108.62597322463989 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.4458\n",
            "Test accuracy on adversarial examples: 0.3233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:23:15,704 cleverhans] Epoch 2 took 109.02508521080017 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5147\n",
            "Test accuracy on adversarial examples: 0.3461\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:25:23,723 cleverhans] Epoch 3 took 109.17684078216553 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5171\n",
            "Test accuracy on adversarial examples: 0.3436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:27:31,867 cleverhans] Epoch 4 took 109.41245675086975 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5341\n",
            "Test accuracy on adversarial examples: 0.3659\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:29:39,933 cleverhans] Epoch 5 took 109.21888971328735 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5577\n",
            "Test accuracy on adversarial examples: 0.3592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:31:46,895 cleverhans] Epoch 6 took 108.37378478050232 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5857\n",
            "Test accuracy on adversarial examples: 0.3615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:33:54,445 cleverhans] Epoch 7 took 108.35283899307251 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.5930\n",
            "Test accuracy on adversarial examples: 0.3580\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:36:02,106 cleverhans] Epoch 8 took 108.54078793525696 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6039\n",
            "Test accuracy on adversarial examples: 0.3858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:38:09,757 cleverhans] Epoch 9 took 109.15243244171143 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6036\n",
            "Test accuracy on adversarial examples: 0.3732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:40:19,069 cleverhans] Epoch 10 took 110.3404712677002 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6160\n",
            "Test accuracy on adversarial examples: 0.3707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:42:29,485 cleverhans] Epoch 11 took 111.27725648880005 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6267\n",
            "Test accuracy on adversarial examples: 0.3664\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:44:39,137 cleverhans] Epoch 12 took 110.5227632522583 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6075\n",
            "Test accuracy on adversarial examples: 0.3842\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:46:46,600 cleverhans] Epoch 13 took 108.78536987304688 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6124\n",
            "Test accuracy on adversarial examples: 0.3814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:48:54,746 cleverhans] Epoch 14 took 109.258061170578 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6446\n",
            "Test accuracy on adversarial examples: 0.3801\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:51:03,782 cleverhans] Epoch 15 took 110.1007444858551 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6157\n",
            "Test accuracy on adversarial examples: 0.3743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:53:12,622 cleverhans] Epoch 16 took 110.15545964241028 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6336\n",
            "Test accuracy on adversarial examples: 0.3764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:55:20,412 cleverhans] Epoch 17 took 108.51104092597961 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6397\n",
            "Test accuracy on adversarial examples: 0.3824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:57:29,420 cleverhans] Epoch 18 took 110.10432648658752 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6460\n",
            "Test accuracy on adversarial examples: 0.3813\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 19:59:37,339 cleverhans] Epoch 19 took 109.05764102935791 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6410\n",
            "Test accuracy on adversarial examples: 0.3912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 20:01:45,552 cleverhans] Epoch 20 took 109.16668939590454 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6510\n",
            "Test accuracy on adversarial examples: 0.3874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 20:03:53,981 cleverhans] Epoch 21 took 109.71366238594055 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6520\n",
            "Test accuracy on adversarial examples: 0.3839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 20:06:02,622 cleverhans] Epoch 22 took 109.7910087108612 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6501\n",
            "Test accuracy on adversarial examples: 0.3864\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 20:08:11,551 cleverhans] Epoch 23 took 109.88018608093262 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6363\n",
            "Test accuracy on adversarial examples: 0.3899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2020-01-22 20:10:20,039 cleverhans] Epoch 24 took 109.60943078994751 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy on legitimate examples: 0.6682\n",
            "Test accuracy on adversarial examples: 0.3879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cleverhans.utils.AccuracyReport at 0x7fd508e5c6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKDjR1LkPJlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}